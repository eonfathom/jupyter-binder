{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Michael's Minimal Recurrent Neural Network in Julia\n",
    "This is an implementation of the simplest RNN model presented in Andrei Karpathy's awesome article, [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), written in [Julia](http://julialang.org/).\n",
    "\n",
    "Karpathy presents a 100-line [\"minimal recurrent nerual network\"](https://gist.github.com/karpathy/d4dee566867f8291f086), written in Python. This notebook reimplements the model and training code, aiming less to be short and more to be clear and instructive.\n",
    "\n",
    "The network will be trained with a body of text, character by character, and learn to predict the pattern of that text. We will use it to generate, or \"hallucinate,\" new text that *feels* like the original!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The Data\n",
    "\n",
    "In this case my data is all of Shakespeare's commedies concatenated in a single file, but any large text file will do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 2108622\n",
      "\n",
      " --------- Some Sample Text: -------- \n",
      "\n",
      "\tALL'S WELL THAT ENDS WELL\n",
      "\n",
      "\n",
      "\tDRAMATIS PERSONAE\n",
      "\n",
      "\n",
      "KING OF FRANCE\t(KING:)\n",
      "\n",
      "DUKE OF FLORENCE\t(DUKE:)\n",
      "\n",
      "BERTRAM\tCount of Rousillon.\n",
      "\n",
      "LAFEU\tan old lord.\n",
      "\n",
      "PAROLLES\ta follower of Bertram.\n",
      "\n",
      "\n",
      "Steward\t|\n",
      "\t|  ser\n",
      "\n",
      " --------- ... ---------------------- \n",
      "\n",
      "y mate, that's never to be found again,\n",
      "\tLament till I am lost.\n",
      "\n",
      "LEONTES\tO, peace, Paulina!\n",
      "\tThou shouldst a husband take by my consent,\n",
      "\tAs I by thine a wife: this is a match,\n",
      "\tAnd made between's by vows. Thou hast found mine;\n",
      "\tBut how, is to be question'd; for I saw her,\n",
      "\tAs I thought, dead, and have in vain said many\n",
      "\tA prayer upon her grave. I'll not seek far--\n",
      "\tFor him, I partly know his mind--to find thee\n",
      "\tAn honourable husband. Come, Camillo,\n",
      "\tAnd take her by the hand, whose worth and honesty\n",
      "\tIs richly noted and here justified\n",
      "\tBy us, a pair of kings. Let's from this place.\n",
      "\tWhat! look upon my brother: both your pardons,\n",
      "\tThat e'er I put between your holy looks\n",
      "\tMy ill suspicion. This is your son-in-law,\n",
      "\tAnd son unto the king, who, heavens directing,\n",
      "\tIs troth-plight to your daughter. Good Paulina,\n",
      "\tLead us from hence, where we may leisurely\n",
      "\tEach one demand an answer to his part\n",
      "\tPerform'd in this wide gap of time since first\n",
      "\tWe were dissever'd: hastily lead away.\n",
      "\n",
      "\t[Exeunt]\n"
     ]
    }
   ],
   "source": [
    "original_text = \"\"\n",
    "open(\"all_comedies_cat.txt\") do original_text_file\n",
    "    global original_text\n",
    "    original_text = readall(original_text_file)\n",
    "end\n",
    "# Print some samples from the text, just to get a feel for it.\n",
    "println(\"Length: \", length(original_text))\n",
    "println(\"\\n --------- Some Sample Text: -------- \\n\")\n",
    "println(original_text[1:200])\n",
    "println(\"\\n --------- ... ---------------------- \\n\")\n",
    "print(original_text[end-1000:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Int64} with 69 entries:\n",
       "  'D'  => 18\n",
       "  '|'  => 69\n",
       "  'Y'  => 39\n",
       "  '\\'' => 6\n",
       "  '.'  => 11\n",
       "  'U'  => 35\n",
       "  'B'  => 16\n",
       "  ':'  => 12\n",
       "  ';'  => 13\n",
       "  'J'  => 24\n",
       "  'Z'  => 40\n",
       "  'o'  => 57\n",
       "  'N'  => 28\n",
       "  'p'  => 58\n",
       "  'F'  => 20\n",
       "  'j'  => 52\n",
       "  '!'  => 4\n",
       "  'y'  => 67\n",
       "  'E'  => 19\n",
       "  'r'  => 60\n",
       "  'm'  => 55\n",
       "  'S'  => 33\n",
       "  'A'  => 15\n",
       "  ','  => 9\n",
       "  'T'  => 34\n",
       "  ⋮    => ⋮"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = sort(unique(original_text))  # Converts an index into a character.\n",
    "println(length(alphabet))\n",
    "\n",
    "reverse_alphabet = [ch => i for (i,ch) in enumerate(alphabet)]  # Converts a char into an index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----------\n",
    "## Define the Model\n",
    "\n",
    "This is modelled after Karpathy's minimal RNN, described in his [excellent article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and implemented in this [short gist](https://gist.github.com/karpathy/d4dee566867f8291f086). From the article, the basic step function is:\n",
    "\n",
    "```\n",
    "# update the hidden state\n",
    "self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "# compute the output vector\n",
    "y = np.dot(self.W_hy, self.h)\n",
    "```\n",
    "\n",
    "There are also two biases, `bh` and `by`, used to compute self.h and y. So the model has 5 learned parameters: `Wxh`, `Whh`, `Why`, `bh`, and `by`.\n",
    "\n",
    "Mathematically, each step looks like\n",
    "\n",
    "$$\n",
    "h' = tanh(W_{hh} * h + W_{xh} * x) + b_h\n",
    "$$\n",
    "$$\n",
    "y = W_{hy} * h' + b_y\n",
    "$$\n",
    "\n",
    "where $x$ is the input for the step (a vector that represents a character), $h$ is the hidden state left from the previous step, and $y$ is the predicted output vector (a probability distribution over the set of characters).\n",
    "\n",
    "Each of these vectors represent the \"neurons\" in the neural network; each cell a neuron. $x$ holds the input neurons, $h$ the hidden neurons, and $y$ the output neurons. The connections between these neurons are represented by the weight matrices and bias vectors. For example, the cells in $W_{hx}$ control how heavily each input neuron from $x$ should affect the new value for each hidden neuron in $h$. The weights and biases -- the parameters -- are learned from training the network with successive batches of inputs.\n",
    "\n",
    "### Representing the Network\n",
    "\n",
    "We will represent this network using a Julia type we'll call `RNN`, holding all the parameters (weights+biases) and the hidden state vector (hidden neuron layer). Note that the hidden state is not trained like the parameters, but can is state held by the network during operation. Therefore, we will still store it inside the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0017844250981475835\n",
      " 0.008640373687761155]\n",
      "[0.0\n",
      " 0.0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A Recurrent Neural Network is a collection of weight matrices, which are updated after each training batch,\n",
    "and a \"hidden state\", h, which is updated after each classification in the batch and carried throughout the whole batch.\n",
    "\"\"\"\n",
    "type RNN\n",
    "    # The Weights and Biases (matrices)\n",
    "    Wxh   # Controls how each input neuron affects the new hidden neuron values.\n",
    "    Whh   # Controls how each hidden neuron affects the new hidden neuron values.\n",
    "    Why   # Controls how to calculate the output neurons from the hidden layer.\n",
    "    bh    # Bias added to new hidden state.\n",
    "    by    # Bias added to calculated output vector.\n",
    "\n",
    "    # Adagrad update \"memory\" weights. This is an implementation detail of the Adagrad update algorithm.\n",
    "    mWxh\n",
    "    mWhh\n",
    "    mWhy\n",
    "    mbh\n",
    "    mby\n",
    "    \n",
    "    # The hidden state is updated by each step, and is a part of generating the next step's classification. It is not\n",
    "    # updated by backpropogation (training), but it's part of calculating the new update for the weights after each step.\n",
    "    # Updating the hidden state and carrying it to the next step is called \"unrolling\" the RNN, and is what makes an\n",
    "    # RNN \"deep\", similar to having many hidden layers in a more traditional RNN.\n",
    "    h     # Not a variable, but still associated with the RNN.\n",
    "    \n",
    "    function RNN(in_dim::Integer, hidden_dim::Integer, out_dim::Integer)\n",
    "        # Initialize the Weights/Biases variables\n",
    "        Wxh = randn(hidden_dim, in_dim)*0.01      # scaled way down.\n",
    "        Whh = randn(hidden_dim, hidden_dim)*0.01  # Initialize these with random values from normal distribution,\n",
    "        Why = randn(out_dim, hidden_dim)*0.01\n",
    "        bh = zeros(hidden_dim, 1)\n",
    "        by = zeros(out_dim, 1)\n",
    "        \n",
    "        # Initialize the memory variables\n",
    "        mWxh, mWhh, mWhy, mbh, mby = zeros(Wxh), zeros(Whh), zeros(Why), zeros(bh), zeros(by)\n",
    "\n",
    "        # Initialize the hidden state vector.\n",
    "        h = zeros(bh)\n",
    "        \n",
    "        new(Wxh, Whh, Why, bh, by, mWxh, mWhh, mWhy, mbh, mby, h)\n",
    "    end\n",
    "end\n",
    "# Test code\n",
    "r = RNN(1,2,1)\n",
    "println(r.Wxh)\n",
    "println(r.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the recurrence\n",
    "\n",
    "A recurrent neural net is *recurrent* because the simple model is repeated multiple times to create the overall model, carrying state from each step to the next. This is called \"unrolling\" the net.\n",
    "\n",
    "This matters, because in order for backpropogation to calculate the effect that the hidden state has on the loss, it needs to take the changes from one `state` to another into account when calculating the gradient. If you didn't unroll the network, and instead just returned the `state` from each run and passed the `state` back into the next run, the `state`'s only effect on the cost would be how the `state` effected `y`, not how the current `state` effected the next state.\n",
    "\n",
    "For example, the simple network without unrolling it looks like this:\n",
    "  \n",
    "$  cost = truth - y $, where $ y = W_{hy}*h' $ and $h' = tanh(W_{hh}*h + W_{xh}*x) $\n",
    "\n",
    "The current state, $h$, only effects $cost$ through it's impact on $y$. Even though it sets the *next state*, $h'$, the transition from $h$ to $h'$ is never considered during backpropogation.\n",
    "\n",
    "Instead, an *unrolled* network *does* effect the cost both from the current $h$ and on its effect on the next $h$. Consider a network unrolled for 2 steps, $x_0$ and $x_1$:\n",
    "\n",
    "$  cost = (truth_1 - y_1) + (truth_0 - y_0) $\n",
    "\n",
    "for $ y_1 = W_{hy}*h_2 $, $h_2 = tanh(W_{hh}*h_1 + W_{xh}*x_1) $\n",
    "\n",
    "and\n",
    "$ y_0 = W_{hy}*h_1 $, $h_1 = tanh(W_{hh}*h_0 + W_{xh}*x_0) $\n",
    "\n",
    "Now, $W_{hh}$'s impact on $h_1$ is reflected in the `cost`, alongside its impact on $y_0$.\n",
    "\n",
    "SO, I *think*, the more steps you unroll a network for before doing backpropogation, the more emphasis you're placing on the hidden state's impact versus the other weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3x1 Array{Float64,2}:\n",
       "  1.0\n",
       " -5.0\n",
       "  5.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A helper function to prevent spiraling values when doing derivatives.\n",
    "\"\"\"\n",
    "function clip_values(deltas_matrix, max_threshold::Float64, min_threshold::Float64)\n",
    "    deltas_matrix = max(deltas_matrix, min_threshold)\n",
    "    deltas_matrix = min(deltas_matrix, max_threshold)\n",
    "    return deltas_matrix\n",
    "end\n",
    "# Test code\n",
    "clip_values([1.0 -11.0 6.0]', 5.0, -5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ys:\n",
      "[0.33330945985467153\n",
      " 0.3333195712203447\n",
      " 0.3333709689249837]\n",
      "[0.3333325336240126\n",
      " 0.3333333587436077\n",
      " 0.3333341076323797]\n",
      "[0.3333094638455846\n",
      " 0.3333195728680036\n",
      " 0.3333709632864119]\n",
      "h:\n",
      "[0.005225351549333576\n",
      " 0.019163877595654712\n",
      " -0.002894982134721514\n",
      " 0.00544056088368608]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    forward_pass_step(r::RNN, in_h, x)\n",
    "\n",
    "A single step in the RNN: calculate a new output vector and new hidden\n",
    "state from an input vector (a one-hot vector) and the current hidden state.\n",
    "\n",
    "Returns the output vector as a raw vector and as a normalized\n",
    "vector, which can be thought of as a probability distribution, and the new hidden state.\n",
    "\"\"\"\n",
    "function forward_pass_step(r::RNN, in_h, x)\n",
    "    const local Wxh = r.Wxh\n",
    "    const local Whh = r.Whh\n",
    "    const local Why = r.Why\n",
    "    const local bh = r.bh\n",
    "    const local by = r.by\n",
    "\n",
    "    # Calculate the new h.\n",
    "    const new_h = tanh(Wxh*x + Whh*in_h + bh)\n",
    "\n",
    "    const y = Why*new_h + by\n",
    "    \n",
    "    # --- Normalize y so that it represents a probability distribution over the set of characters.\n",
    "    \n",
    "    # Taking exp(y) conveniently eliminates negative values from tanh.\n",
    "    const exp_y = exp(y)     # 2x1 (pointwise)\n",
    "    const y_norm = exp_y ./ sum(exp(y)) # 2x1 Element wise division \n",
    "\n",
    "    return y_norm, y, new_h\n",
    "end\n",
    "\n",
    "# Test code\n",
    "r1 = RNN(2,4,3)\n",
    "r1.h = [0 0 0 0.]'\n",
    "y1, _, r1.h = forward_pass_step(r1, r1.h, [1.0 0]')\n",
    "y2, _, r1.h = forward_pass_step(r1, r1.h, [0.0 0.1]')\n",
    "y3, _, r1.h = forward_pass_step(r1, r1.h, [1.0 0.]')\n",
    "println(\"ys:\\n$y1\\n$y2\\n$y3\")\n",
    "println(\"h:\\n\",r1.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "[1.9302244965501878],\n",
      "\n",
      "[-0.04238982839396551 -0.0059616523702824\n",
      " -0.054066359497087255 -0.0074679389032205196\n",
      " -0.04334667562232223 -0.009150013099945634],\n",
      "\n",
      "[0.0053131034170606425 -0.012153131298957148 -0.039003515265967854\n",
      " -0.0006733502928015604 -0.015007342671044354 -0.04914214346576764\n",
      " -0.001873647622807225 -0.01320572057049087 -0.04117375097267407],\n",
      "\n",
      "[0.15236661289368483 0.05868962310872297 0.09448040464381108\n",
      " -0.15236661289368486 -0.058689623108723 -0.0944804046438111],\n",
      "\n",
      "[-0.04835148076424791\n",
      " -0.061534298400307774\n",
      " -0.05249668872226786],\n",
      "\n",
      "[0.20836354484521263\n",
      " -0.2083635448452127])\n",
      "\n",
      "[0.3165562782891978\n",
      " 0.34135939193526\n",
      " 0.5251652648834423]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    forward_pass_backpropogate_batch(r::RNN, xs, ts)\n",
    "\n",
    "Unroll the RNN for all the inputs in this batch, then perform backpropogation and calculate the\n",
    "delta updates for its parameters. Returns the `cost` calculated over this batch, and the delta updates.\n",
    "\n",
    "This function modifies r.h, and leaves it in its new state.\n",
    "\n",
    "r - an RNN.\n",
    "x - The batch of input vectors: a tuple of one-hot vectors.\n",
    "t - The batch of \"truth\" vectors: a tuple of one-hot vectors of same count as xs.\n",
    "\"\"\"\n",
    "function forward_pass_backpropogate_batch(r::RNN, xs, ts)\n",
    "\n",
    "    #Forward Pass\n",
    "    hs, ys, ps = Dict(), Dict(), Dict()\n",
    "    hs[0] = r.h\n",
    "    local Wxh = r.Wxh\n",
    "    local Whh = r.Whh\n",
    "    local Why = r.Why\n",
    "    local bh = r.bh\n",
    "    local by = r.by\n",
    "\n",
    "    out_delta_max_threshold = 5.0   # Tweakable.\n",
    "    out_delta_min_threshold = -5.0   # Tweakable.\n",
    "\n",
    "    cost = 0\n",
    "    \n",
    "    # Unroll the network.\n",
    "    # For each input, perform the forward pass step and update the hidden state. Keep track\n",
    "    # of the state after each step so we can calculate the derivatives correctly during backpropogation.\n",
    "    for i in 1:length(ts)\n",
    "        local h = hs[i-1]\n",
    "        local x = xs[i]\n",
    "        local t = ts[i]\n",
    "\n",
    "        ps[i], ys[i], hs[i] = forward_pass_step(r, h, x)\n",
    "\n",
    "        # Take the dot-product with t-inverse, to get only\n",
    "        # the value from log(ps[i]) which corresponds to the truth.\n",
    "        prediction_for_truth_value = t' * ps[i]  # 1x1 (scalar-ish)\n",
    "\n",
    "        # The log-cost reflects how close the prediction for the truth value was to 1.\n",
    "        cost += -log(prediction_for_truth_value)\n",
    "    end\n",
    "\n",
    "    # ----------  Now go back down:\n",
    "    # Starting from the cost, calculate the derivative (gradient) of the cost, with respect to each variable,\n",
    "    # or, *how much the cost changes if you change each variable*. This allows us to move each variable along\n",
    "    # it's derivative to lower the cost for this batch.\n",
    "\n",
    "    # These will be our final outputs, they represent ``δcost/δvariable``.\n",
    "    dWxh = zeros(r.Wxh)\n",
    "    dWhh = zeros(r.Whh)\n",
    "    dWhy = zeros(r.Why)\n",
    "    dbh  = zeros(r.bh)\n",
    "    dby  = zeros(r.by)\n",
    "    \n",
    "    ∂cost_∂hnext = zeros(r.h)\n",
    "    for i in length(ts):-1:1\n",
    "\n",
    "        # Copied from Karpathy. I don't know what these two lines mean.\n",
    "        #dy = ps[i]\n",
    "        #dy -= ts[i] # backprop into y\n",
    "        #dWhy += dy * hs[i]'\n",
    "        #dby += dy\n",
    "        #dh = Why' * dy + dhnext # backprop into h\n",
    "        #dhraw = (1 - hs[i] .* hs[i]) .* dh # backprop through tanh nonlinearity\n",
    "        #dbh += dhraw\n",
    "        #dWxh += dhraw * xs[i]'\n",
    "        #dWhh += dhraw * hs[i-1]'\n",
    "        #dhnext = Whh' * dhraw\n",
    "        \n",
    "        \n",
    "        ∂cost_∂y = ps[i]\n",
    "        ∂cost_∂y -= ts[i] # backprop into y\n",
    " \n",
    "        ∂cost_∂Why = ∂cost_∂y * hs[i]'  # 2x3\n",
    "        ∂cost_∂by = ∂cost_∂y            # 2x1\n",
    " \n",
    "        ∂cost_∂h = Why' * ∂cost_∂y  +  ∂cost_∂hnext   # 3x1\n",
    "        dhraw = (1 - hs[i] .^ 2) .* ∂cost_∂h    # \"backprop through tanh nonlinearity\"\n",
    " \n",
    "        ∂cost_∂bh = dhraw  # 3x1    # noop\n",
    " \n",
    "        ∂cost_∂Whh = dhraw * hs[i-1]'   # 3x3\n",
    "        ∂cost_∂Wxh = dhraw * xs[i]'   # 3x2\n",
    " \n",
    "        ∂cost_∂hnext = Whh' * dhraw  # 3x1\n",
    " \n",
    "        # Update the final derivatives\n",
    " \n",
    "        dWxh += ∂cost_∂Wxh\n",
    "        dWhh += ∂cost_∂Whh\n",
    "        dWhy += ∂cost_∂Why\n",
    "        dbh  += ∂cost_∂bh\n",
    "        dby  += ∂cost_∂by\n",
    "\n",
    "\n",
    "        #println(\"∂cost_∂Wxh:$∂cost_∂Wxh\")\n",
    "    end\n",
    "\n",
    "    # Clip deltas to mitigate exploding gradients.\n",
    "    dWxh = clip_values(dWxh, out_delta_max_threshold, out_delta_min_threshold) \n",
    "    dWhh = clip_values(dWhh, out_delta_max_threshold, out_delta_min_threshold)\n",
    "    dWhy = clip_values(dWhy, out_delta_max_threshold, out_delta_min_threshold)\n",
    "    dbh = clip_values(dbh, out_delta_max_threshold, out_delta_min_threshold)\n",
    "    dby = clip_values(dby, out_delta_max_threshold, out_delta_min_threshold)\n",
    "     \n",
    "    # Update the hidden state from this run.\n",
    "    r.h = hs[length(ts)]\n",
    "\n",
    "    return cost, dWxh, dWhh, dWhy, dbh, dby \n",
    "\n",
    "end\n",
    "\n",
    "r = RNN(2,3,2)\n",
    "r.Wxh = [.5 .2 ; 0.1 0.1 ; 0.2 0.2]\n",
    "r.Whh = [.1 .1 .1 ; .2 .2 .2 ; 0.3 0.3 .3]\n",
    "r.Why = [.4 .5 .6; .7 .8 .9 ]\n",
    "r.h = [0.4 .2 0.8]'\n",
    "println(forward_pass_backpropogate_batch(r, ([1 0]', [0 1]', [0 1]'), ([0 1]', [1 0]', [0 1]')))\n",
    "println(\"\\n\",r.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "The last part of the model is the most important part for expirementing! These are the settings for the RNN we will use to learn our text. These are *not* the `parameters`, or variables, learned inside the network, but rather the options for setting up the network.\n",
    "\n",
    "- `hidden_size` -- How many neurons to have in the hidden layer.\n",
    "- `seq_length` -- How many characters to include in a single training batch. This translates to *how many steps to unroll the network during training*, which, remember is the crucial detail that makes an RNN recurrent. The larger this is, the more data it will take to train, but it will possibly be able to learn features that are more spread out in time (such as opening and closing brackets, maybe). On the other hand, if this is tiny, it will get feedback for each character and can learn with far less data, but might not learn spelling/structure as well.\n",
    "- `learning_rate` -- How much to adjust each parameter along the gradients, or derivatives, calculated from backpropogation. If this is too large, the model can swing wildly and never settle. We use an \"Adagrad update\" which slowly shrinks the `learning_rate` over time, so it can start a bit larger than one would do without using adagrad.\n",
    "\n",
    "Play with these to see how the network changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparamaters\n",
    "hidden_size = 100           # In Karpathy's min-char-rnn.py, this is set to 100.\n",
    "seq_length = 50             # In Karpathy's min-char-rnn.py, this is set to 25.\n",
    "learning_rate = 1e-1        # In Karpathy's min-char-rnn.py, this is set to 1e-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training!\n",
    "---------\n",
    "Now let's do the training!\n",
    "\n",
    "Concept:\n",
    "\n",
    "Calculate the gradient of the Cost with respect to each of the weights, i.e. the partial derivates $\\partial$Cost / $\\partial$Weight for each weight.\n",
    "\n",
    "This gradient represents how much the Cost would change if we update the weights a tiny bit. So we subtract the gradient from the weight to decrease the Cost!\n",
    "\n",
    "Note: We don't even care what $y$ is, we only use it to calculate the gradient for a given set of weights. So that information can stay in the backpropogate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.8513927492820152][0.5499489269489144\n",
      " 0.32445312710513785\n",
      " 0.5042017549258068]\n",
      "[2.7732563220837685][0.2733504302372906\n",
      " -0.1218558122595025\n",
      " -0.03313777320467589]\n",
      "[0.3673099058722714 0.10641030522686429\n",
      " -0.026636732258434413 0.006942351574688553\n",
      " 0.06798023469772287 0.11654742554190307][0.014194231453489218 0.02240190796108634 0.015019502381031684\n",
      " 0.11765274426740126 0.12166334880633656 0.11498598332296982\n",
      " 0.22201671040103732 0.22457719522088102 0.2173526964776151][0.4753469640869792 0.5827236545826828 0.6514414354375764\n",
      " 0.6246530359130209 0.7172763454173171 0.8485585645624236][-0.11227657394104021\n",
      " -0.11147721367622182\n",
      " -0.11101694366027448][0.021547450149868995\n",
      " -0.021547450149868946]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    update(r, xs, ts)\n",
    "\n",
    "Train on a batch of inputs and truths, and update the model.\n",
    "\n",
    "Performs Adagrad update gradient descent using the gradients calculated from backpropogation.\n",
    "\n",
    "r - RNN\n",
    "xs - a batch of inputs\n",
    "ts - a batch of truths (as one-hot nx1 matrices)\n",
    "\"\"\"\n",
    "function update(r, xs, ts)\n",
    "    loss, ∂cost_∂Wxh, ∂cost_∂Whh, ∂cost_∂Why, ∂cost_∂bh, ∂cost_∂by = forward_pass_backpropogate_batch(r,xs,ts)\n",
    "    \n",
    "    # Adagrad update (Gradient Descent)\n",
    "    # As these terms grow, the shift below becomes smaller, because they're the denominator.\n",
    "    r.mWxh += ∂cost_∂Wxh .^ 2\n",
    "    r.mWhh += ∂cost_∂Whh .^ 2\n",
    "    r.mWhy += ∂cost_∂Why .^ 2\n",
    "    r.mbh += ∂cost_∂bh .^ 2\n",
    "    r.mby += ∂cost_∂by .^ 2\n",
    "    \n",
    "    # Shift the weights down along their gradients (derivates).\n",
    "    # Remember that ∂cost_∂Wxh specifies how much cost will *increase* with an\n",
    "    # increase in Wxh, so to decrease cost, you would move along negative ∂cost_∂Wxh.\n",
    "    r.Wxh -= learning_rate * ∂cost_∂Wxh  ./ sqrt(r.mWxh + 1e-8)\n",
    "    r.Whh -= learning_rate * ∂cost_∂Whh  ./ sqrt(r.mWhh + 1e-8)\n",
    "    r.Why -= learning_rate * ∂cost_∂Why  ./ sqrt(r.mWhy + 1e-8)\n",
    "    r.bh -= learning_rate * ∂cost_∂bh    ./ sqrt(r.mbh + 1e-8)\n",
    "    r.by -= learning_rate * ∂cost_∂by    ./ sqrt(r.mby + 1e-8)\n",
    "    \n",
    "    return loss\n",
    "end\n",
    "\n",
    "# Test Code:\n",
    "r = RNN(2,3,2)\n",
    "r.Wxh = [.5 .2 ; 0.1 0.1 ; 0.2 0.2]\n",
    "r.Whh = [.1 .1 .1 ; .2 .2 .2 ; 0.3 0.3 .3]\n",
    "r.Why = [.4 .5 .6; .7 .8 .9 ]\n",
    "r.h = [0.4 .2 0.8]'\n",
    "loss = update(r, ([1 0]', [0 1]', [0 1]', [1 0]'), ([0 1]', [1 0]', [0 1]', [1 0]'))\n",
    "println(loss, r.h)\n",
    "\n",
    "update(r, ([1 0]', [0 1]', [0 1]', [1 0]'), ([0 1]', [1 0]', [0 1]', [1 0]'))\n",
    "update(r, ([1 0]', [0 1]', [0 1]', [1 0]'), ([0 1]', [1 0]', [0 1]', [1 0]'))\n",
    "loss = update(r, ([1 0]', [0 1]', [0 1]', [1 0]'), ([0 1]', [1 0]', [0 1]', [1 0]'))\n",
    "\n",
    "println(loss, r.h)\n",
    "\n",
    "println(r.Wxh, r.Whh, r.Why, r.bh, r.by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    make_one_hot(length, index)\n",
    "\n",
    "A helper function to make a \"one-hot\" vector, or a vector which is\n",
    "all zeros with a one at the specified index. We will use this to\n",
    "represent a character as input to the neural network.\n",
    "\"\"\"\n",
    "function make_one_hot(length, index)\n",
    "    v = zeros(Float64, length, 1)\n",
    "    v[index] = 1.0\n",
    "    return v\n",
    "end\n",
    "x = make_one_hot(length(alphabet), reverse_alphabet['.'])\n",
    "assert(1 == x[reverse_alphabet['.']])\n",
    "assert(0 == x[reverse_alphabet['a']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Because Julia doesn't seem to have a function to sample from a set of probabilities?\n",
    "\n",
    "\"\"\"\n",
    "    rand_uniform(a, b)\n",
    "\n",
    "Returns a number from the uniform random distribution between a and b.\n",
    "\"\"\"\n",
    "function rand_uniform(a, b)\n",
    "    a + rand()*(b - a)\n",
    "end\n",
    "\"\"\"\n",
    "    SampleFrom(probabilities)\n",
    "\n",
    "Returns an index between 0 and length(probabilities), chosen based on the provided stepwise probabilities.\n",
    "\"\"\"\n",
    "function SampleFrom(probabilities)\n",
    "\n",
    "    # Sum to create CDF:\n",
    "    cdf = Array(Float64, 0)\n",
    "    sum = 0.0\n",
    "    for p in probabilities\n",
    "        push!(cdf, sum + p)\n",
    "        sum = cdf[end];\n",
    "    end\n",
    "        \n",
    "    # Choose from CDF:\n",
    "    cdf_value = rand_uniform(0.0,cdf[end])\n",
    "    index = searchsortedfirst(cdf, cdf_value);\n",
    "\n",
    "    return index;\n",
    "\n",
    "end\n",
    "# Test Code: run this repeatedly to see how the randomness follows the given distributions.\n",
    "println(SampleFrom([1 1 1 1]))\n",
    "println(SampleFrom([1 1 10 10]))\n",
    "println(SampleFrom([0.1 0.2 0.7 0.8]))\n",
    "#println(hist([SampleFrom([1 1 10 10]) for i in 0:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X;,xOnOqEdxtHRom,[.nbj[w,:o[qCW;FK!dIF&qzvyE]\\n-rzBWclnZ]ykmSM[MP?:ses\\nk)X|JhZuE|wV.qGZwKna,Kmd&U[\\ntO\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    hallucinate(r, seed_idx, num_chars)\n",
    "\n",
    "Generates, \"hallucinates,\" text from the given RNN by repeatedly\n",
    "1. running the network with an input,\n",
    "2. generating an output probability distribution,\n",
    "3. randomly selecting a new index from that distribution,\n",
    "4. and using that index as a new input.\n",
    "It does this num_chars times, to generate text num_chars long.\n",
    "\"\"\"\n",
    "function hallucinate(r, seed_idx, num_chars)\n",
    "    hallucination = \"\"\n",
    "    prev_ids = [seed_idx]\n",
    "    # Should we clear the hidden state (not the weights)?\n",
    "    # Note that Karpathy doesn't modify h, but I think it\n",
    "    # makes sense to start over before each hallucination...\n",
    "    # I don't do it here, though, to keep with his implementation.\n",
    "    for x in range(1,num_chars)\n",
    "        x_vec = make_one_hot(length(alphabet), prev_ids[end])\n",
    "        y_norm,y,r.h = forward_pass_step(r, r.h, x_vec)\n",
    "\n",
    "        # Now sample from y!\n",
    "        letter_idx = SampleFrom(y_norm')\n",
    "        #letter_idx = indmax(y)\n",
    "        char = alphabet[letter_idx]\n",
    "        \n",
    "        append!(prev_ids,[letter_idx])\n",
    "        hallucination = \"$hallucination$char\"\n",
    "    end\n",
    "    return hallucination\n",
    "end\n",
    "r = RNN(length(alphabet), 500, length(alphabet))\n",
    "hallucinate(r, rand(1:length(alphabet)), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----------\n",
    "\n",
    "# Run it!\n",
    "\n",
    "Alright! Let's run it! We set up a loop below which iterates through the text, one batch of `seq_length` characters at a time. For each batch, it runs `update`, which trains the RNN by iterating through the batch, backpropogating, and updating the weights using gradient descent. Each training step needs to operate on a batch, because unrolling the RNN for all the characters in the batch is what makes the Recurrent Neural Network *deep* -- and, as I understand it, being deep is what makes backpropogation/gradient descent somehow magically be a decently bowl-shaped space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_rnn = RNN(length(alphabet), hidden_size, length(alphabet))\n",
    "chars_trained = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Iteration: 0, Chars Trained:0/2108622 Cost: 211.70532522986298 ===========\n",
      "======== Iteration: 0, Chars Trained:0/2108622 Cost: 211.70532522986298 ===========\n",
      "ME[DAAE(q \n",
      "kf|t;FK)!nhb ?'oxVUHBkhl|Rphv-mI;ymIRQ!vr&qrWsFcmn[[|\n",
      "kz.t\tBSzv|w;jrJQIM'GPDejI;e:lb)xv.Q\n",
      "======== Iteration: 100, Chars Trained:5000/2108622 Cost: 213.8570015065917 ===========\n",
      "A  lalsn\n",
      "LNfatsZkIf\tsrh strgsdPlElo, tNin st.T!to oo l gHd)tllUuBthekas hih al oht Hnrv,KZ\tevtai;,ie\n",
      "======== Iteration: 200, Chars Trained:10000/2108622 Cost: 209.00623347906495 ===========\n",
      "eoy evaeelbowriwoe itp rss ctaun bel tyee antotht:y\n",
      "\n",
      "PJRSAA\tTluhomgeig\n",
      "\n",
      "GLAHSlJ T\tmhdis,\n",
      "\t(fAi| wtt \n",
      "======== Iteration: 300, Chars Trained:15000/2108622 Cost: 203.6009864389416 ===========\n",
      "kls se ma t'veu\n",
      "\tqr ooock t verenurymy\n",
      "\tKluy cheidm mivuvcgand sheien tbuyf n\n",
      "\n",
      "\tZIve|\n",
      "!\n",
      "NEjEREC&\tris\n",
      "======== Iteration: 400, Chars Trained:20000/2108622 Cost: 197.56577771053566 ===========\n",
      "uy\n",
      "\n",
      "\twcos nop gowM,\n",
      "\ton goctetiud her,,\n",
      "\tmnung siard\n",
      "\t.hatnurelatef sethivird houodss fos domn uw fh\n",
      "======== Iteration: 500, Chars Trained:25000/2108622 Cost: 191.3216734081336 ===========\n",
      "!A\n",
      "AVtu uuisr w ybe dox lod , ol ovos\n",
      "\tAyoe Was lou Wp oad werl I oin, hosseste tothe Ire th:\tathee \n",
      "======== Iteration: 600, Chars Trained:30000/2108622 Cost: 185.84826012935508 ===========\n",
      "g.\n",
      "\n",
      "\n",
      "REI\n",
      "CLM\n",
      "\n",
      "\n",
      "KEHU\n",
      "\tAxIt Ln Cod isn hoaud pl hed to tl ikrith ird an word, tit ooit if:\n",
      "\n",
      "FAUEOA(S\tI\n",
      "======== Iteration: 700, Chars Trained:35000/2108622 Cost: 180.19605589311746 ===========\n",
      "ha wrinveathe lryilpes that\tal with motehon soave,\n",
      "\n",
      "KLENN\tALEPKLESTher ghee kire isulet: nthaheabrea\n",
      "======== Iteration: 800, Chars Trained:40000/2108622 Cost: 175.09025388213294 ===========\n",
      "s,\n",
      "\n",
      "\tI We]\n",
      "\n",
      "CO INE-g\n",
      "\tingat mine. Iey wheprlom mfibecqy : hou .\n",
      "FELP Poe! at witin:,\n",
      "\tCrNngs, an\tne \n",
      "======== Iteration: 900, Chars Trained:45000/2108622 Cost: 170.14325034871132 ===========\n",
      " fer oulsord iamI\tre: I m|ias, f eroBel.\n",
      "\n",
      "\n",
      "EIN Tith;\n",
      "\n",
      "HTRiny her thofs.\tme and be, elve:\n",
      "\tAro seny s\n",
      "======== Iteration: 1000, Chars Trained:50000/2108622 Cost: 165.39511077517136 ===========\n",
      "t alwikine mheud haee, ind int. wangvino's aul thandcowes thhe\tCyo corperist oud tho\n",
      "\tI nyve the, be\n",
      "======== Iteration: 1100, Chars Trained:55000/2108622 Cost: 160.77320795045682 ===========\n",
      "vony anpaig yowr; ang, who\n",
      "\tiws: ig to dur have iy, ant sovmy amt whiivendeCs hemen\n",
      "\tI mant he delt \n",
      "======== Iteration: 1200, Chars Trained:60000/2108622 Cost: 156.4589812288314 ===========\n",
      "ory thave ase a soale\n",
      "\tI mal wory Ism, I\n",
      "\t[Erat you my you keres\n",
      "\tOrteice, br mave yo hlilesere ifer\n",
      "======== Iteration: 1300, Chars Trained:65000/2108622 Cost: 152.88097981938478 ===========\n",
      " jowy thi cforant namshe.\n",
      "\n",
      "HELATRES\tNlan wonpanty wmalnonnofemen. I thin eatses\n",
      "\tThee stave, sist in\n",
      "======== Iteration: 1400, Chars Trained:70000/2108622 Cost: 149.77912613943167 ===========\n",
      "win Heknad, on te'd las's and wande\n",
      "\tWone\n",
      "\tTe vve her t:icy.\n",
      "\n",
      "PEURTES\tTero horr pelac thish.\n",
      "\n",
      "\tbe ti\n",
      "======== Iteration: 1500, Chars Trained:75000/2108622 Cost: 146.95364584628564 ===========\n",
      "ongNe kitethigepy Wellothe, aml he hithuem fond wormI te minsrongt f're ware tie to gicim I he senat\n",
      "======== Iteration: 1600, Chars Trained:80000/2108622 Cost: 144.24648130400243 ===========\n",
      "ut told faxe wo fo whay tord.\n",
      "\n",
      "vun\tirloe |oosh youndRthe, onot tor whe\n",
      "\thang Ford,\n",
      "\tThaken: sewing a\n",
      "======== Iteration: 1700, Chars Trained:85000/2108622 Cost: 141.5928205788066 ===========\n",
      "ing will, youg.\tI tooms hinuwe'tintheserind,:\n",
      "\to wtousour gou and wilk, ae,\n",
      "\twhed au,\n",
      "\tarare ar;e at\n",
      "======== Iteration: 1800, Chars Trained:90000/2108622 Cost: 139.1134779503908 ===========\n",
      "ovy bilcond; thee so, ankeaned, enas, inar, neans ordowhigicon atho, fisdpen\n",
      "\twe shothir, dous seman\n",
      "======== Iteration: 1900, Chars Trained:95000/2108622 Cost: 136.65885446225613 ===========\n",
      "t ind\n",
      "\tYofse hoactocet aver I an of fil her's whoche L ot sallothess icg o honent icak the solthec a\n",
      "======== Iteration: 2000, Chars Trained:100000/2108622 Cost: 134.61367979902923 ===========\n",
      "rd\n",
      "\tifsuer,\n",
      "\tfanvange kord.\n",
      "\n",
      "AFoER\tOud is, beandur thacy alad\tenve, shr here parsuast, thond,, Save \n",
      "======== Iteration: 2100, Chars Trained:105000/2108622 Cost: 132.3368118106737 ===========\n",
      "iMt, tediwar thislesensslrmad butres at\n",
      "\tsparath,\n",
      "\tGo foest mea-dy] will korfaand cod and t tis coll\n",
      "======== Iteration: 2200, Chars Trained:110000/2108622 Cost: 130.67934217626095 ===========\n",
      "athy sule thit, , shat'n he ke hikerall hut semares ule?\n",
      "\n",
      "\n",
      "\t[Enl womun an huthe vers he hat med\n",
      "\tto \n",
      "======== Iteration: 2300, Chars Trained:115000/2108622 Cost: 128.9813294812178 ===========\n",
      "n t\n",
      "\tThit ierter ins, !\n",
      "\tCf ieny iu.\n",
      "\n",
      "COLENEHLLES\thint is ind a grat that kely mare indi: do nos lrI\n",
      "======== Iteration: 2400, Chars Trained:120000/2108622 Cost: 127.32012388468006 ===========\n",
      "se toncogee yoNt as hepledh. Ling'mased so an cist a fEren min  I hust lhow kAl;\n",
      "\tPhe sesfurthot pro\n",
      "======== Iteration: 2500, Chars Trained:125000/2108622 Cost: 126.01061322727801 ===========\n",
      "e mavebeves at saaved, yabd stid, youd in mea 'cevefterstvios bovey aturt kre ind dast, my.\n",
      "\n",
      "\tGeave\n",
      "\n",
      "======== Iteration: 2600, Chars Trained:130000/2108622 Cost: 124.51918229889212 ===========\n",
      " ny firt ine le wot whet thiger hae fourd hest nofe, wull ulve henesCy\n",
      "\t[tome theivned al wo: Ie non\n",
      "======== Iteration: 2700, Chars Trained:135000/2108622 Cost: 122.67787930788096 ===========\n",
      "yive at is ree'st now: the gay Onve dou, lins tour sidsespards]\n",
      "\n",
      "KI[NBEx]\n",
      "\n",
      "BAUFEUSt af fouas shou eg\n",
      "======== Iteration: 2800, Chars Trained:140000/2108622 Cost: 122.58315973102667 ===========\n",
      "for welce bur llou me at ind my, whimh that of yout Iw; Int mion Eygun wat nes cave: bech thitu isr.\n",
      "======== Iteration: 2900, Chars Trained:145000/2108622 Cost: 121.70547722411784 ===========\n",
      "e\n",
      "\tSly the\n",
      "\tar o fate ait\n",
      "\tHer ant\n",
      "\tmint thef poms at\n",
      "\n",
      "OLLES\tsee th shanof youl you lo my andurt te \n",
      "======== Iteration: 3000, Chars Trained:150000/2108622 Cost: 120.82530648129435 ===========\n",
      "og\n",
      "\tOo seill\n",
      "\tPaf youd\n",
      "\tFhis not ind and tild whaM tor, ach bratere me.\n",
      "\n",
      ")TAGH AROULES\tWE'lf\n",
      "\tDurl c\n",
      "======== Iteration: 3100, Chars Trained:155000/2108622 Cost: 119.91177187272778 ===========\n",
      ", Inou shere nay: rl ien lened that hos brtined,\n",
      "\tBhlen an. I saves she me, krlllome fadd nomd, at s\n",
      "======== Iteration: 3200, Chars Trained:160000/2108622 Cost: 118.98106208882638 ===========\n",
      " me, Bik not thes at Cith bureve am hilt thath thend.\n",
      "\n",
      "IA\n",
      "\n",
      "FES;\n",
      "\tI me sout ad feat thim, dhered mere\n",
      "======== Iteration: 3300, Chars Trained:165000/2108622 Cost: 118.23893559800023 ===========\n",
      "elllie soat?\n",
      "\n",
      "BERTOLIRBS\tAus he sunder out se, sears freirse,\n",
      "\twetere, att hir my and comurs thaind\n",
      "\n",
      "======== Iteration: 3400, Chars Trained:170000/2108622 Cost: 117.63799287784093 ===========\n",
      "eeserser andw thim pames mengerd ufolk brves\n",
      "\tlafst of haond uiinithet thed comy mave isue unhiuth a\n",
      "======== Iteration: 3500, Chars Trained:175000/2108622 Cost: 116.96377034436502 ===========\n",
      "es ther j or and and gome,\n",
      "\taxpar da ber.\n",
      "\n",
      "\tOund beret bune wondrt\n",
      "\tdar onf heakn\n",
      "\tAf sey, tpeemrs f\n",
      "======== Iteration: 3600, Chars Trained:180000/2108622 Cost: 116.40206254218747 ===========\n",
      "ore anger liqifenket;\n",
      "\tHare's lull be'trlobfises.\n",
      "\n",
      "\n",
      "UQExtES\twhe hee syme vives make, the them rind o\n",
      "======== Iteration: 3700, Chars Trained:185000/2108622 Cost: 115.79530445071038 ===========\n",
      "ic me\n",
      "\tAnde,\n",
      "\tAndened hes loy hin\n",
      "\ttofandeacotse soty bpyateteill and at what I I ther pedantyoCcove\n",
      "======== Iteration: 3800, Chars Trained:190000/2108622 Cost: 115.2532358239015 ===========\n",
      "hate, pre, PUKI I patip kepe contos; may, wit bred,\n",
      "\tTrate, mantuetif, me.\n",
      "\n",
      "OUVIE\tWhy bind thamhere \n",
      "======== Iteration: 3900, Chars Trained:195000/2108622 Cost: 114.71588422311973 ===========\n",
      "CUKIINA\tAns but.\n",
      "\n",
      "\tO I FoO\n",
      "\tI the thellefeid, ast at; wifasteret\n",
      "\tThe lfand't bot is rin: wintthe an\n",
      "======== Iteration: 4000, Chars Trained:200000/2108622 Cost: 114.05322030576998 ===========\n",
      "ct veit lovet!\n",
      "\tAk\n",
      "\tAn hivee sheed oon mint, I hat I thike; une the and.\n",
      "\n",
      "FELIA\tT wratred?\n",
      "\n",
      "ROOLLIAD\n",
      "======== Iteration: 4100, Chars Trained:205000/2108622 Cost: 113.05407141693483 ===========\n",
      "n\n",
      "\tyou speatp nerener\n",
      "\tJhey tim, on thend, mo lis yourest thee sout thave ll shond vers for, ipre go\n",
      "======== Iteration: 4200, Chars Trained:210000/2108622 Cost: 112.46996642204311 ===========\n",
      "ll thou as eme,, bel'ssusy\n",
      "\tend youl to shaup:?\n",
      "\n",
      "JShachest I calk and garest ad I but er hor hall om\n",
      "======== Iteration: 4300, Chars Trained:215000/2108622 Cost: 112.02445284579701 ===========\n",
      "t likh, wnat prith: come wreckher be teilse r thille.\n",
      "\n",
      "ROSALLI\n",
      "\tNe fore ensum not not Frethes wis of\n",
      "======== Iteration: 4400, Chars Trained:220000/2108622 Cost: 111.59443153630178 ===========\n",
      "\tPhe pred ins fore pkont, you, lion sor? AT A\n",
      "CELTYim with youces, thice couk burf. Fishter theavatn\n",
      "======== Iteration: 4500, Chars Trained:225000/2108622 Cost: 110.91725457701318 ===========\n",
      "hathe iurs a y halloucke tome say om hisparns a duad thave of weos arw dime we hean't\n",
      "\twomy pfome he\n",
      "======== Iteration: 4600, Chars Trained:230000/2108622 Cost: 109.81480040973271 ===========\n",
      "s- .\n",
      "\n",
      "HALANDOS\n",
      "\tO thiuth hGe mocedel dondar: mare my rericks fouth willl I\n",
      "\tthoud thounbyionde, thow\n",
      "======== Iteration: 4700, Chars Trained:235000/2108622 Cost: 109.50669429285178 ===========\n",
      "r, to I shough turis till\n",
      "\tboup ass not the thoul whard shal is then he-betthen\n",
      "\tMand ape't sor; oun\n",
      "======== Iteration: 4800, Chars Trained:240000/2108622 Cost: 109.21399919520046 ===========\n",
      "sd, shipis rad you hitle ou'gly, caon tilc, dwiegle it we wist.\n",
      "\n",
      "|HE IVLSEYA\tA deextearches fiin: ar\n",
      "======== Iteration: 4900, Chars Trained:245000/2108622 Cost: 108.98975373515711 ===========\n",
      "\n",
      "\tBhersagino anid the he kerouce hained in Zakd gokelcee.\n",
      "\tThe lomeet the keof tis seapiget! forthit\n",
      "======== Iteration: 5000, Chars Trained:250000/2108622 Cost: 108.62162228671126 ===========\n",
      " emy, IImy nongy\n",
      "\tYou sho, thin Go sarde fo, ? I me anither obe:s, to\n",
      "\tInghever tteen of im.\n",
      "\n",
      "ROSELL\n",
      "======== Iteration: 5100, Chars Trained:255000/2108622 Cost: 108.37631489270163 ===========\n",
      "e me'thos?' fallfenss, your carr tast fomess. Irpotay, thim thith lingome,\n",
      "\tsureusante; wos thins hi\n",
      "======== Iteration: 5200, Chars Trained:260000/2108622 Cost: 108.30383418009914 ===========\n",
      "se cawicoshat.\n",
      "\n",
      "DRMUAU\n",
      "\twid shave, folle.\n",
      "\tand now Lo'd I red ny ith you with ind theie to preete: f\n",
      "======== Iteration: 5300, Chars Trained:265000/2108622 Cost: 109.47867833063422 ===========\n",
      "rs.\n",
      "\n",
      "OOLESASG\tY\n",
      "\tNT mave likee:.\n",
      "\n",
      "TOS\n",
      "\tSERILANDONE INI bim,test hit bin(w ch, teven\tep.\n",
      "\n",
      "DUKEwMS YO \n",
      "======== Iteration: 5400, Chars Trained:270000/2108622 Cost: 109.58478591907286 ===========\n",
      "] den\n",
      "\tAighave,\n",
      "\tThtans the fid fol wisicoe\n",
      "\tI tict purcupurs and\n",
      "\tAnte purcoun, to ulal come to tha\n",
      "======== Iteration: 5500, Chars Trained:275000/2108622 Cost: 109.42930105281354 ===========\n",
      "Te ly win worlow thee ave ato his beate thepled arr-wers; fok that my mat that dame meacefser?\n",
      "\n",
      "\tTa \n",
      "======== Iteration: 5600, Chars Trained:280000/2108622 Cost: 109.36377109884234 ===========\n",
      " seod so hy hom? i see aleur. Wo tir'g 'loN, me the hel\n",
      "\tbaising epe mer.\n",
      "\n",
      "AYOANE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tFof no frar m\n",
      "======== Iteration: 5700, Chars Trained:285000/2108622 Cost: 107.9389849229739 ===========\n",
      "shines to lare I thell\n",
      "\tTheagion I qure?\n",
      "BIFON\tmerlie! iglage ford sey, und it in thut baing, may,\n",
      "\t\n",
      "======== Iteration: 5800, Chars Trained:290000/2108622 Cost: 107.29368608675857 ===========\n",
      "p.\n",
      "\n",
      "ANTIF PFCoPROMUS\n",
      "\n",
      "\t[E EFESE OFLVCOUS\tShel in. Andrer the wousge forven Ippancest, aNll Spoment-s\n",
      "======== Iteration: 5900, Chars Trained:295000/2108622 Cost: 106.12613944450425 ===========\n",
      "BUCUS\tWhate atw cour eren, yoan, it.\n",
      "\n",
      "ANThe'd she\n",
      "\tan ith wor; I mot thag] Cedpersse anther., a dor,\n",
      "======== Iteration: 6000, Chars Trained:300000/2108622 Cost: 106.07339142684886 ===========\n",
      "ROFUS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FVHEGIO,\n",
      "\tDhoullg; orgulle\n",
      "\twold ice my sin dos kis you dove wyou deder: is, mith ins maly\n",
      "======== Iteration: 6100, Chars Trained:305000/2108622 Cost: 104.81725590813139 ===========\n",
      "fill? Sonkige I,\n",
      "\n",
      "\tAndI  sumour, in ond\n",
      "\tIrlaghof boos coull fool wnoce a heee,\n",
      "\tAndhe,\n",
      "\tAnt I giy h\n",
      "======== Iteration: 6200, Chars Trained:310000/2108622 Cost: 104.10934098399558 ===========\n",
      "e diletsD\tssmand my and mip?s, in wid ate thell diy oths wout byior tike ving.\n",
      "\n",
      "ANGowh\n",
      "\tFnotr hich b\n",
      "======== Iteration: 6300, Chars Trained:315000/2108622 Cost: 103.91391394920109 ===========\n",
      "'sI-f the whekethazand yourssaceredt, I for bold, rest me stay, coabe'd mans in Iall ma thand, hen w\n",
      "======== Iteration: 6400, Chars Trained:320000/2108622 Cost: 103.4129438923808 ===========\n",
      "uliin\tthest ald I me aus wid coull.\n",
      "\n",
      "JROMIO OF SYRAMOROF OFlousf ach cut's folfe me neswer je you pi\n",
      "======== Iteration: 6500, Chars Trained:325000/2108622 Cost: 102.67169942304376 ===========\n",
      "OLUSOORO\n",
      "\n",
      "\tCemallhestoovion the dersue sere that th thme shinh hire whiik pondy saaver, my bire\n",
      "\tqee\n",
      "======== Iteration: 6600, Chars Trained:330000/2108622 Cost: 102.62324877391737 ===========\n",
      "\n",
      "\tGome in igh with has serie.\n",
      "\n",
      "ANTIPHOLUs\n",
      "\tHay ferwear DRADROSOROF SYRACUSE\tAug rcomo so, dalfoond i\n",
      "======== Iteration: 6700, Chars Trained:335000/2108622 Cost: 102.79492156687117 ===========\n",
      " doinud ors ast, goa,\n",
      "\tAnd sind do thes at hestthestret me abes rint fory had had his pave come him \n",
      "======== Iteration: 6800, Chars Trained:340000/2108622 Cost: 102.92716437219673 ===========\n",
      "houltend houn\n",
      "\tAnte the manter\n",
      "\tWhepethanve bryes fleiss tibe bersere?\n",
      "\tAfne, his vicht coud gow.\n",
      "\n",
      "A\n",
      "======== Iteration: 6900, Chars Trained:345000/2108622 Cost: 102.97831738203388 ===========\n",
      "nd verbtol my lost. The sibefalos, I ir, Ievighelelse mune dey\n",
      "\tAn me, simustiy her sim, nates, dich\n",
      "======== Iteration: 7000, Chars Trained:350000/2108622 Cost: 102.2875734900595 ===========\n",
      " me.\n",
      "\n",
      "ADRIPUCHESUS\tTo gay im ousuuld ans zout, finl:\n",
      "\tThy me mur,\n",
      "\tMith sa mish arne tat himm tousto\n",
      "======== Iteration: 7100, Chars Trained:355000/2108622 Cost: 103.97371612633108 ===========\n",
      "oow come thnom and liken\n",
      "\tOo the.\n",
      "\n",
      "DRMIPUSLO RONG OF YRICHESUS\tWE GE IFESDROMI\n",
      "\tShech for cusaxt har\n",
      "======== Iteration: 7200, Chars Trained:360000/2108622 Cost: 104.82041544804807 ===========\n",
      "ate thou tercicating his this lo be is woulevin my diwe,\n",
      "\tIm spent: the afe.\n",
      "\n",
      "OMII\n",
      "\tHn Bevist, the P\n",
      "======== Iteration: 7300, Chars Trained:365000/2108622 Cost: 105.22601907035272 ===========\n",
      "yoM or be to shie thight o'th lratd are haigh has you havet niow him how. PThiog eier so hourl ey, a\n",
      "======== Iteration: 7400, Chars Trained:370000/2108622 Cost: 105.57559427386119 ===========\n",
      "ut a gotsolous] benist winged aple'thankes vore hiveus thar to maker\twats, sond ist bity\tsour, thith\n",
      "======== Iteration: 7500, Chars Trained:375000/2108622 Cost: 105.81522026203106 ===========\n",
      "t makeung bith]\n",
      "\n",
      "\n",
      "\t[EwYRUSURIN\tLengair: eruig! I sir will he gorsh noLl\n",
      "\n",
      "\tT yow wade\n",
      "\thistles ay iru\n",
      "======== Iteration: 7600, Chars Trained:380000/2108622 Cost: 106.17373356416414 ===========\n",
      "\n",
      "OGEN\tWelus werade all tiles ester lratue.\n",
      "\tThish meeven: toanighid memand!\n",
      "\n",
      "TIAIION\tTe willd shesev\n",
      "======== Iteration: 7700, Chars Trained:385000/2108622 Cost: 106.2914559371618 ===========\n",
      " seraltwend cou is? Cath lpay as storacl veettarus it DROMIO CTOMENHIMO\tWhe the you, patierby lad sc\n",
      "======== Iteration: 7800, Chars Trained:390000/2108622 Cost: 106.31843092410654 ===========\n",
      "' lloisoren her I mane buome of whithen ous fater hirn;\n",
      "\tI'd not fat ry?\n",
      "\n",
      "IUE Fanto,\n",
      "\tDare on torke \n",
      "======== Iteration: 7900, Chars Trained:395000/2108622 Cost: 106.70076165821384 ===========\n",
      "r ewen loy whyr.\n",
      "\n",
      "IMO CEShis cord, tn ibe torlles, eristonon I it, pitur, ecan thou speacothay.\n",
      "\n",
      "Fai\n",
      "======== Iteration: 8000, Chars Trained:400000/2108622 Cost: 106.51302126640947 ===========\n",
      "in't pudin& Il the ofpe: for peavens,\n",
      "\n",
      "\n",
      "CSxut at rome.\n",
      "\n",
      "CUSUS\tNol.\n",
      "\n",
      "TACANG\tSat ptatisett\tInow'll som\n",
      "======== Iteration: 8100, Chars Trained:405000/2108622 Cost: 106.20100758996007 ===========\n",
      ".\n",
      "\n",
      "\n",
      "\tThe kighis ar thauthertither ymanne.\n",
      "\n",
      "COLUS\tI' a padsy.\n",
      "\n",
      "\n",
      "COFEUS\tBingane, dorgheranld core lath\n",
      "======== Iteration: 8200, Chars Trained:410000/2108622 Cost: 106.2151730173175 ===========\n",
      "nath--I wilf phim make whave ain that mese or; to prkern\tAnd tiding mey. Iadment Qupnuut?\n",
      "\n",
      "\n",
      "P[ETIANI\n",
      "======== Iteration: 8300, Chars Trained:415000/2108622 Cost: 106.64844704472466 ===========\n",
      "e dondbachiled weaces; low lo dake in hisiscerde,\n",
      "\tHis'd healad? of th e lan, ther oorsth a do-her y\n",
      "======== Iteration: 8400, Chars Trained:420000/2108622 Cost: 106.95503623807933 ===========\n",
      "e: Mandist elbed it sens feage to al the Lyes\n",
      "\tI she prens thang?\n",
      "\n",
      "DORIMALINUSENI And I toue lavoser\n",
      "======== Iteration: 8500, Chars Trained:425000/2108622 Cost: 107.18089957930925 ===========\n",
      "yelve, lold.\n",
      "\n",
      "SSKCETASTLIMUS\n",
      "OVBEZEN\tI dlick theay tham he fad\n",
      "\tLurd-etiince: and thay mho't wat end\n",
      "======== Iteration: 8600, Chars Trained:430000/2108622 Cost: 107.00923559196936 ===========\n",
      "me pos a toon prosbith nok as ethar saie ntist of hath seamevs googheack foung of me. I thy mubous\n",
      "\t\n",
      "======== Iteration: 8700, Chars Trained:435000/2108622 Cost: 107.0128096939682 ===========\n",
      ") my;\n",
      "\tAnd us the cu,\n",
      "\tIf sory, as here's' Sy exeress.\n",
      "\n",
      "CiThB I  my a wrave-fA pray buten. Cold\n",
      "\tHat\n",
      "======== Iteration: 8800, Chars Trained:440000/2108622 Cost: 106.67833277829226 ===========\n",
      "RTIMIONE\tNho that Rond you the manctertow to stere ontore errty him Willo zas secter: I thank the th\n",
      "======== Iteration: 8900, Chars Trained:445000/2108622 Cost: 106.72626594961007 ===========\n",
      "ng now moe\n",
      "\tPhoochacrePamp' yow.\n",
      "\tThem, the harde comy I wivee yourd;\n",
      "\tRortinge\n",
      "\tthe kinlt.\n",
      "\n",
      "CLO[E-\t\n",
      "======== Iteration: 9000, Chars Trained:450000/2108622 Cost: 106.91044102709589 ===========\n",
      "is hel the de the hay!-f rerpind me.\n",
      "\tfouth, fall bet.\n",
      "\n",
      "OLIPINGENe a tise! Is yournes' to ceave so o\n",
      "======== Iteration: 9100, Chars Trained:455000/2108622 Cost: 106.5323625369502 ===========\n",
      "antinbi, of beand,\n",
      "\tI nithte se me sorf hand an and yo bate,\n",
      "\tThow\n",
      "\tGint CoWh,\n",
      "\tIn bbe\n",
      "\tThes, cot ha\n",
      "======== Iteration: 9200, Chars Trained:460000/2108622 Cost: 106.4204053901892 ===========\n",
      "e\n",
      "\tThath bath owe yof mone\n",
      "\tTo malle alf'ld's Praw scaunk day, arussts fity thou fait and stere,\n",
      "\tMi\n",
      "======== Iteration: 9300, Chars Trained:465000/2108622 Cost: 106.5486557207987 ===========\n",
      "ithen' sit is''s, brenast.\n",
      "\n",
      "\t[Ont in shavis am blour ther live ufe tho the ghalow Frre fouting, pork\n",
      "======== Iteration: 9400, Chars Trained:470000/2108622 Cost: 106.36928438656503 ===========\n",
      "pray lomele\n",
      "\tThat mate, a grees than\tshe.\n",
      "\n",
      "DUKENHION\t|\n",
      "\tT S ylur. Hof courst,\n",
      "\n",
      "\tNomt anr whon lesmer\n",
      "======== Iteration: 9500, Chars Trained:475000/2108622 Cost: 106.10016582914722 ===========\n",
      "enefhers,\n",
      "\tThem tisn and horderte wellsee wand, I's houbl os is estard me set cumd newe my maar do m\n",
      "======== Iteration: 9600, Chars Trained:480000/2108622 Cost: 106.21253178485287 ===========\n",
      "ik hod hes, bevele bord thysm wit as\n",
      "\tWhos,\n",
      "\tNA\n",
      "\tNeeven tulu.\n",
      "\n",
      "\t[Exece! of my you: CThoumf feare sti\n",
      "======== Iteration: 9700, Chars Trained:485000/2108622 Cost: 106.52659968280886 ===========\n",
      "intsherlouns\n",
      "\tFore fiken to combun agams me,\n",
      "\tFreswes the hardresbewer wnon, o rerantma\n",
      "\tThapgr thou\n",
      "======== Iteration: 9800, Chars Trained:490000/2108622 Cost: 106.93999661894722 ===========\n",
      "y then cho applaro-ucime of-eings af and.\n",
      "\n",
      "INE CELI SMOLUENg bring aje\n",
      "\tme that you lut nean;\n",
      "\tSesti\n",
      "======== Iteration: 9900, Chars Trained:495000/2108622 Cost: 106.65437044583004 ===========\n",
      "not onverothey dene'dly a ly\n",
      "\tgore hace: be; wiin won ourge homf senter entseutir then, I wo chille\n",
      "\n",
      "======== Iteration: 10000, Chars Trained:500000/2108622 Cost: 106.2313089905358 ===========\n",
      "\tOf thatl, a l, witth it hdre;\n",
      "\tBut soue and be not, Och boLexst\n",
      "\tWith be lo doop'splama lrorle lide\n",
      "======== Iteration: 10100, Chars Trained:505000/2108622 Cost: 106.03229151191417 ===========\n",
      "frecing Pothis ltsee pand I mave and--O'dven: mens son her or likes.\n",
      "\n",
      "PAIUS IT I Iblown Lerelie\n",
      "\tFre\n",
      "======== Iteration: 10200, Chars Trained:510000/2108622 Cost: 105.47113212667787 ===========\n",
      ": thou. Baof her duid uu lur!,\n",
      "\tBenir furs; and men shos torus. More?\n",
      "\n",
      "IRIANI\n",
      "\n",
      "OPHADDUM\tBs rom, I jr\n",
      "======== Iteration: 10300, Chars Trained:515000/2108622 Cost: 105.44788780244173 ===========\n",
      "eald good he: mine, thy hald her airs!\n",
      "\tI whis a orherper\n",
      "\tArom.\n",
      "\n",
      "ARIVIRIA\tYo f so liren\n",
      "\tAnd pracea\n",
      "======== Iteration: 10400, Chars Trained:520000/2108622 Cost: 106.13917001233537 ===========\n",
      "d\n",
      "\tTh ghis ham my ridex\n",
      "\tList seree, that sore seess. Wo liditger ben of arde be,\n",
      "\tWhere stoldars' h\n",
      "======== Iteration: 10500, Chars Trained:525000/2108622 Cost: 106.24442762846982 ===========\n",
      " condirs har sor exccever\n",
      "\tas wo lis momas oler, of IYot either:\n",
      "\tWqouth vast bote!\n",
      "\n",
      "\tI -ey wheakt g\n",
      "======== Iteration: 10600, Chars Trained:530000/2108622 Cost: 106.45310255564017 ===========\n",
      " Bettowger whow,\n",
      "\tcerlee, ips ary sile pend sisear!\n",
      "\tDo lirnt tho has sheares will thor sarens your \n",
      "======== Iteration: 10700, Chars Trained:535000/2108622 Cost: 106.11600362231663 ===========\n",
      "LOPDE\tnith to so, wued maowr thas of the\n",
      "\n",
      "\tThe trighterg, shy ule vend sad of' than mpple,\n",
      "\tMaro.\n",
      "\n",
      "I\n",
      "======== Iteration: 10800, Chars Trained:540000/2108622 Cost: 106.39370762849755 ===========\n",
      " in sare koreted han hing.\n",
      "\n",
      "PICHOS\tWhad dhe row: mialtherme obtitell to uname ponding blaktilw, elle\n",
      "======== Iteration: 10900, Chars Trained:545000/2108622 Cost: 106.18125585357348 ===========\n",
      " them\n",
      "\tFadedl\tdayt now boter'ds gowst this earaqthek my yofe make m wall hean tit seand--\n",
      "\tA ll youl\n",
      "======== Iteration: 11000, Chars Trained:550000/2108622 Cost: 105.84715519542793 ===========\n",
      "\n",
      "\tPonge reatht of air any igll\tdisher, soen, sheere hid in poreher rong ath wheadten to ther, thes d\n",
      "======== Iteration: 11100, Chars Trained:555000/2108622 Cost: 105.3380144699329 ===========\n",
      "en?\n",
      "\n",
      "MOTH\tS t ury velly, wove forll lomedst trike nome so sussurettoy this aonome.\n",
      "\n",
      "BIVADADO\tWhit lu\n",
      "======== Iteration: 11200, Chars Trained:560000/2108622 Cost: 105.3542462535991 ===========\n",
      "sir longenit.\n",
      "\n",
      "\n",
      "Ke he thecaltery!\n",
      "\n",
      "DON\n",
      "ANIANIDOS MOYRINO\tDe a do poye winuthers] rabeangonger, t a m\n",
      "======== Iteration: 11300, Chars Trained:565000/2108622 Cost: 105.26154168344178 ===========\n",
      "im his rith erpey.\n",
      "\n",
      "OR[ARd I\n",
      "\tARIAN\tSier thy shay saroge ther bewere muper and should-st bed of ipl;\n",
      "======== Iteration: 11400, Chars Trained:570000/2108622 Cost: 105.73493149474812 ===========\n",
      "ere thene hend.\n",
      "\tO   ol tho pay love that secen.\n",
      "\n",
      "MOOFE DRINARDEDNE win the cabl.\n",
      "\n",
      "ARON]\n",
      "\n",
      "SDERONDORD\n",
      "======== Iteration: 11500, Chars Trained:575000/2108622 Cost: 106.32213556156267 ===========\n",
      ".\n",
      "AMO; of liig-iengime they!\n",
      "\t's merye.\n",
      "\n",
      "COSTENERD\tO' andreemy, ine has; en; tot us pexy;\n",
      "\tMame\n",
      "\tded\n",
      "======== Iteration: 11600, Chars Trained:580000/2108622 Cost: 106.4593271941405 ===========\n",
      "RPoTH\tNiepne seetron frithe.\n",
      "\n",
      "HoTh weede\n",
      "\tWarvgres with daokestre:\n",
      "\tmathar bleoro' wheadker\tsheingha\n",
      "======== Iteration: 11700, Chars Trained:585000/2108622 Cost: 106.23477004264522 ===========\n",
      "geapten of uol he, youjt it mare that wist soin not\n",
      "\tFreppy the in a ward.\n",
      "\n",
      "BIFERMDELON\n",
      "HANFERDONGEL\n",
      "======== Iteration: 11800, Chars Trained:590000/2108622 Cost: 105.98808830347977 ===========\n",
      "or the prome trel! weace shat soain?\n",
      "\n",
      "DRONAN\tTuke trerat\tstines,\n",
      "\tAes sids of ertaike prarave heand'\n",
      "======== Iteration: 11900, Chars Trained:595000/2108622 Cost: 106.45041325307074 ===========\n",
      "pead wheare tr, the uon the versus sad ither theet]\n",
      "\n",
      "JOTENIN\tAed waitse oll, thece tomuold fou ine; \n",
      "======== Iteration: 12000, Chars Trained:600000/2108622 Cost: 106.35515009728869 ===========\n",
      " to nofle,\n",
      "\tYou hat it wheme shou; 'llpuntrene shoous lorume py yor my guy in the amI put him meoly,\n",
      "======== Iteration: 12100, Chars Trained:605000/2108622 Cost: 106.0432655092967 ===========\n",
      " that theebe!\n",
      "\n",
      "PION\tShy, seare, mop, a neart thaund meanty tsare cand, mes forr,\n",
      " Buke havy homanate\n",
      "======== Iteration: 12200, Chars Trained:610000/2108622 Cost: 105.71564961082481 ===========\n",
      ",\n",
      "\tWit pampeedickt;\n",
      "\t'the mant rest;\n",
      "\tSo youh by men boked wilmours,\n",
      "\tAine bet:\n",
      "\tAilet?  ilo my\n",
      "\thar\n",
      "======== Iteration: 12300, Chars Trained:615000/2108622 Cost: 105.0718479171848 ===========\n",
      "Theme foue-rimees woux telllosputhelte measce?\n",
      "\n",
      "ROSDORDE\tTor the arusboveters: wontiginand-jes.\n",
      "\n",
      "DRL\n",
      "======== Iteration: 12400, Chars Trained:620000/2108622 Cost: 104.66088522280651 ===========\n",
      "he thouut the aty hemey,\n",
      "\tYour suth mee thyer,\n",
      "\tMcheaio gors ond\n",
      "\tINd prreviend:\n",
      "\tTo matks.\n",
      "\n",
      "ROSALLE\n",
      "======== Iteration: 12500, Chars Trained:625000/2108622 Cost: 104.17516790779176 ===========\n",
      "i fou plade oorr, Dide, is bpork in; ther theed.\n",
      "\tI poave wour maase l Longear tablous to were hrers\n",
      "======== Iteration: 12600, Chars Trained:630000/2108622 Cost: 103.93525209840976 ===========\n",
      "or thes our to the pall wee here of wony meony; he to mall lie'se so-muchsert wollows ef; icr wiect]\n",
      "======== Iteration: 12700, Chars Trained:635000/2108622 Cost: 103.71630857129539 ===========\n",
      "y lutt mikhe crter you Judred]\n",
      "\n",
      "DUMOS\tButher cowing preser.\n",
      "\tHe ay, exsw, to t at, hest wridn orp fo\n",
      "======== Iteration: 12800, Chars Trained:640000/2108622 Cost: 103.5582845219285 ===========\n",
      " to the pise mate the wave Lrite brarks o weld of lad to comof not In whoblaon  ncees progl of datti\n",
      "======== Iteration: 12900, Chars Trained:645000/2108622 Cost: 103.47937938836674 ===========\n",
      " so seno'd was with ad ollabt heaind I meald and came the diggh a wros in st butoust will sorse,\n",
      "\tOS\n",
      "======== Iteration: 13000, Chars Trained:650000/2108622 Cost: 104.37570982025741 ===========\n",
      ",\n",
      "\n",
      "PiIT\tWhall macesletorgh lored LUCOSTES\tMoM yato aire save oudcion\tdelde undier of to trors and Lu\n",
      "======== Iteration: 13100, Chars Trained:655000/2108622 Cost: 104.7124088939821 ===========\n",
      " a whith not meat, yourd; thay bed tright sainclies the hall waiz sy to clousane ant the qegall oup,\n",
      "======== Iteration: 13200, Chars Trained:660000/2108622 Cost: 104.99160450561811 ===========\n",
      "\n",
      "\tHere my I cherdy and to teil are won ain lillied not thereirepliok tpenin\n",
      "\t('tlikn l, mapin,\n",
      "\tAnd \n",
      "======== Iteration: 13300, Chars Trained:665000/2108622 Cost: 105.00237836616303 ===========\n",
      "cincery bouthire havvy,\n",
      "\tonisum' deme n, berise the andes\n",
      "\tAl os sir; to pnows node to sikhy bucjou,\n",
      "======== Iteration: 13400, Chars Trained:670000/2108622 Cost: 105.18614397567963 ===========\n",
      "ping, deed, it sinthn En wro? here ables; anin!.\n",
      "\n",
      "LUCA\tIF whiy you, that it deand?\n",
      "\n",
      "SALUS\tAre frrwei\n",
      "======== Iteration: 13500, Chars Trained:675000/2108622 Cost: 104.7708962505966 ===========\n",
      "a the vex'll your ous yous theredm gatobelin thout hemar to VE\n",
      "POCUM\tEuths wirbl, your the Gyer the \n",
      "======== Iteration: 13600, Chars Trained:680000/2108622 Cost: 104.4527040437254 ===========\n",
      "Phou sely weragind.\n",
      "\n",
      "PDMANEL\tWeach.\n",
      "\t[phe konwer wompe coor's you he\n",
      "\tGent ing thoth for not A\n",
      "\n",
      "\n",
      "\tTh\n",
      "======== Iteration: 13700, Chars Trained:685000/2108622 Cost: 104.13890382423838 ===========\n",
      "Trecent Iist and IAnstme.\n",
      "\n",
      "LUCIO\tFerove the kilf.\n",
      "\n",
      "\n",
      "\n",
      "\tAC Eumantous and af no d atk duate sheire,\n",
      "\twa\n",
      "======== Iteration: 13800, Chars Trained:690000/2108622 Cost: 103.98402637935679 ===========\n",
      "thay lispboigef, Ba mes no\n",
      "\tjonget this, tom shall ofror roseafte conte wempe:\n",
      "\tWhat I Frienpe your \n",
      "======== Iteration: 13900, Chars Trained:695000/2108622 Cost: 103.45563290749229 ===========\n",
      "d\n",
      "\t'om equy it]\n",
      "\n",
      "LUSAEVEYSlo\n",
      "\tSot.\n",
      "\n",
      "ANNELEL I wall\n",
      "\tTo Iupon me\n",
      "\tInt ofred!\n",
      "\tPall dies, untione the \n",
      "======== Iteration: 14000, Chars Trained:700000/2108622 Cost: 103.34595819271097 ===========\n",
      " eadens red hadtonge Pitre thy so hath ticcotrs\n",
      "\tfore bno.\n",
      "\n",
      "LIENDE\tThe dhut ir th wletore are gedive\n",
      "======== Iteration: 14100, Chars Trained:705000/2108622 Cost: 103.16652671999427 ===========\n",
      "LUON\n",
      "\tThee! I pingoonace so sicinestsosursen'cders lfour?\n",
      "\tINamint, buty.\n",
      "\n",
      "\n",
      "FENGELLE\tAnd of uf fuisk\n",
      "======== Iteration: 14200, Chars Trained:710000/2108622 Cost: 102.86169579615935 ===========\n",
      "ikgerte fa is\n",
      "\tell seoveronk\n",
      "\tgree thut so? The rantim frremlay?\n",
      "\n",
      "IUBEO\tThy hee!\n",
      "\n",
      "DUMION\tThe sukher!\n",
      "======== Iteration: 14300, Chars Trained:715000/2108622 Cost: 103.01418210707398 ===========\n",
      " bogher, whom whal, in am nobut plade? Pfeisg suplay wathe ally the. fuoped, this bavens,\n",
      "\tthat a lo\n",
      "======== Iteration: 14400, Chars Trained:720000/2108622 Cost: 102.64312574940452 ===========\n",
      "To VUKTVINENETI\tCo doud ms hapall fekir!\n",
      "\n",
      "LUKE VINNE-TIAt LUKB IO NE AnO VINCENI\tOr E Phiur wit le't\n",
      "======== Iteration: 14500, Chars Trained:725000/2108622 Cost: 102.58998867069133 ===========\n",
      "hay, is sirs elbe ins and needher fith at, an Vo'm hich mick diter,\n",
      "\tI\n",
      "\tAfruth blomeer heard whencre\n",
      "======== Iteration: 14600, Chars Trained:730000/2108622 Cost: 102.57647788458326 ===========\n",
      "erpous boves youp the feak, in in\n",
      "\tMeit prosistronn, will of galplys\n",
      "\tqueriogetat you brbot, is camu\n",
      "======== Iteration: 14700, Chars Trained:735000/2108622 Cost: 102.49722440952924 ===========\n",
      " canvays.\n",
      "\n",
      "\tCorks, I burce lios; mestanding blid] be.\n",
      "\n",
      "\t[ExtoSTANDELLA\tAnd hell the a girtuy not sha\n",
      "======== Iteration: 14800, Chars Trained:740000/2108622 Cost: 102.35958726610147 ===========\n",
      "rls.\n",
      "\n",
      "POM\tPay\n",
      "\t. mast ise: this\n",
      "\tsward whe raiber p ent bes? hement so cafurun't he that chin tut in\n",
      "======== Iteration: 14900, Chars Trained:745000/2108622 Cost: 101.84253218599821 ===========\n",
      "ord DU h Ig AnJUL\tA doute; to from condees, you her muhh\n",
      "\tProte bet besyes tineded's have: fore this\n",
      "======== Iteration: 15000, Chars Trained:750000/2108622 Cost: 101.77806218831675 ===========\n",
      "\tShe heraless\n",
      "\tI meetnst entered dueret seet dyoury and to I crost AnT FE ARLACLA] Wheve the tleatr \n",
      "======== Iteration: 15100, Chars Trained:755000/2108622 Cost: 101.53507509992464 ===========\n",
      "n\n",
      "\tMy pranifibes,\n",
      "\tIne your that se you thand'd didhe.\n",
      "\n",
      "DUmUnO, yit the dukent you hony thaok. Afid.\n",
      "======== Iteration: 15200, Chars Trained:760000/2108622 Cost: 101.21557361831334 ===========\n",
      "ld's dit,  so sent and youn shonem to steer you\n",
      "\tsir,\n",
      "\tMy\n",
      "\tkad beects thom parfe the   the. MATplous\n",
      "======== Iteration: 15300, Chars Trained:765000/2108622 Cost: 100.90729110653096 ===========\n",
      "]\n",
      "\n",
      "\n",
      "\tCord-\tCored, and the prothtong be to ere swat. That fauneis my that the the sthe toiter.\n",
      "\n",
      "ILOCE\n",
      "======== Iteration: 15400, Chars Trained:770000/2108622 Cost: 100.6434313359607 ===========\n",
      "\tWhenom, goves east, sor out wouldon vaind Thither braithe ualt'ed,\n",
      "\tmast a deat: but sifs your ly g\n",
      "======== Iteration: 15500, Chars Trained:775000/2108622 Cost: 100.33984757684053 ===========\n",
      "ad.\n",
      "\n",
      "RIO\tVorion he mort.\n",
      "\n",
      "LUCIO\tNuth ha her cowCliave\n",
      "\tMy.\n",
      "\n",
      "\t[Enelith't\n",
      "\tArd-ajakent,\n",
      "\tAnd Porded\n",
      "\tL\n",
      "======== Iteration: 15600, Chars Trained:780000/2108622 Cost: 101.37728941807518 ===========\n",
      "sy in have diss repongrensidnsse ler not Fliok theme, horly O'lt will your twe my ands to blamen is \n",
      "======== Iteration: 15700, Chars Trained:785000/2108622 Cost: 101.62939843447339 ===========\n",
      "gt hade fords grotosues! feage, he, of If in, yout Gowhe.\n",
      "\n",
      "ENTUBO\tNay neve wiung but dord, is my wor\n",
      "======== Iteration: 15800, Chars Trained:790000/2108622 Cost: 101.87085246408314 ===========\n",
      "?\n",
      "\tBrem:\n",
      "\tGod Jome seuselget,\n",
      "\tes the dace as he weresself the all nak' veat they you.\n",
      "Fhave forlfey\n",
      "======== Iteration: 15900, Chars Trained:795000/2108622 Cost: 102.35011993803333 ===========\n",
      "pratsal dith with's, as to dhas thay home of whean be quupt.\n",
      "\n",
      "INIO\tISinevent noux, thee wouthin: you\n",
      "======== Iteration: 16000, Chars Trained:800000/2108622 Cost: 102.5783086598538 ===========\n",
      "ikn you Jakesh, thin wity, ma the\n",
      "\tThat berting a made oords\n",
      "\tWhis cogestond my facks.\n",
      "\tTs of this f\n",
      "======== Iteration: 16100, Chars Trained:805000/2108622 Cost: 103.26986804577461 ===========\n",
      "andont seaent, shyre.\n",
      "\n",
      "CUKC II\tA Hast, sik. Beblots of sher: DUKE my nowe tiend, I poncalses.\n",
      "\n",
      "ANCIN\n",
      "======== Iteration: 16200, Chars Trained:810000/2108622 Cost: 103.104327772631 ===========\n",
      "\n",
      "\tTo niove Fo like oves on vay. Carver caveils of and guoned ow arker, heare shiset sarver.\n",
      "\n",
      "LONION\n",
      "\n",
      "======== Iteration: 16300, Chars Trained:815000/2108622 Cost: 103.18787266703127 ===========\n",
      " to rereit.\n",
      "\n",
      "KETINE LYMABINOIN\tCw and--S woner.\n",
      "\n",
      "\tI my a in stou,\n",
      "\tgout thoun: to theted.\n",
      "\n",
      "\t[ER\tAnd \n",
      "======== Iteration: 16400, Chars Trained:820000/2108622 Cost: 103.31175493780006 ===========\n",
      "wold Sose, Jownim.\n",
      "\n",
      "SARINO\tWiGl stachive,\n",
      "\tBut shat tist bessertlesisplored that, he ame his you the\n",
      "======== Iteration: 16500, Chars Trained:825000/2108622 Cost: 103.46201215009827 ===========\n",
      "tor the fen that will your this horefiom but manin thene! If eray, sar ding arre I haves fathin: af \n",
      "======== Iteration: 16600, Chars Trained:830000/2108622 Cost: 103.50605180529378 ===========\n",
      " vivirtows\n",
      "\tI wist Nlar'n ged thinossenof\n",
      "\tFyou I  that wive stis me:\n",
      "\tThie fordI\n",
      "\tFraths, as ind ha\n",
      "======== Iteration: 16700, Chars Trained:835000/2108622 Cost: 103.51139272225986 ===========\n",
      "lent, meetwald eniblougs, senter: wo beecloo the foulo. Gouset and thee.\n",
      "\n",
      "\t[Exaint, tlay brenle exe\n",
      "\n",
      "======== Iteration: 16800, Chars Trained:840000/2108622 Cost: 103.56572603214953 ===========\n",
      "for, the, grrote!\n",
      "\n",
      "LALANIO\tWhat be vent you ank\n",
      "\tLer I at!  Frlmsedg the and entwis me dold mand to \n",
      "======== Iteration: 16900, Chars Trained:845000/2108622 Cost: 103.71765554579694 ===========\n",
      "ovy day or me\n",
      "\tDipts, I sall mor!,\n",
      "\tAnzes?\n",
      "\n",
      "BOLILO, A\n",
      "\tweit the: 'oU hather' that the wous thimelken\n",
      "======== Iteration: 17000, Chars Trained:850000/2108622 Cost: 103.35766909197227 ===========\n",
      "my a grepay ree to bee if hall to buch not to mie the deofum.\n",
      "\n",
      "\t[Esty,\n",
      "\tI\n",
      "\tBy sheaisse.\n",
      "\n",
      "\n",
      "\tBut, ming\n",
      "======== Iteration: 17100, Chars Trained:855000/2108622 Cost: 103.00581984450366 ===========\n",
      " of lovith, oof pellickes know your mien'Gt\n",
      "\tgo dumarnerongy theung eritink. Of ofranise conker.\n",
      "\n",
      "LA\n",
      "======== Iteration: 17200, Chars Trained:860000/2108622 Cost: 102.75074659388869 ===========\n",
      "t II : me\n",
      "\tDayatay thou eur. Io piare wind Is of, beranist is dae o'stome, Lake, yatr, thee a has wh\n",
      "======== Iteration: 17300, Chars Trained:865000/2108622 Cost: 102.48552259523547 ===========\n",
      "sys shome\n",
      "\tWhat theaose to penten thee that chathime tuehe.\n",
      "\n",
      "ARIACLOS NTHO\tNuchey they this bontesue\n",
      "======== Iteration: 17400, Chars Trained:870000/2108622 Cost: 102.39662522471279 ===========\n",
      "noinoy, your four hy brepoose\n",
      "\tFor's fwist berrelom; a fexcanat your a-staimemy a laked tuene the ha\n",
      "======== Iteration: 17500, Chars Trained:875000/2108622 Cost: 101.97896133816744 ===========\n",
      " the Gogone imant.\n",
      "\n",
      "-HTAnbing a wheleg mutg I hover be ttuace ite\n",
      "\tfor havers;\n",
      "\tOf ta surioved youn,\n",
      "======== Iteration: 17600, Chars Trained:880000/2108622 Cost: 101.41845830198328 ===========\n",
      "s, seetly the dole, this offmat the jaic, of of of whoe. ho whis if of anon thue camn stay, ca,\n",
      "\tBut\n",
      "======== Iteration: 17700, Chars Trained:885000/2108622 Cost: 101.12059917639942 ===========\n",
      " burdman for sfur is Iy hovite denceet merand\n",
      "\tIall dust.\n",
      "\tThen he is it hime or and uung pring youn\n",
      "======== Iteration: 17800, Chars Trained:890000/2108622 Cost: 101.04153329736313 ===========\n",
      "owe.\n",
      "\tPume stucotwerd encesger theacainsely oftherl, the darctore caon thut deely, wnozece the jurgi\n",
      "======== Iteration: 17900, Chars Trained:895000/2108622 Cost: 100.52329453160608 ===========\n",
      "but thind a cred hingy beduding,\n",
      "\tOn bedion outhest'nd nigtoget she wellher, it onthin thal up, a we\n",
      "======== Iteration: 18000, Chars Trained:900000/2108622 Cost: 101.45613410389109 ===========\n",
      "d marn freaty; to be ou, caze. I k'detrens, hatl, sore; mor it.\n",
      "\n",
      "PORT\n",
      " LAKI\tI ther you thenou sto th\n",
      "======== Iteration: 18100, Chars Trained:905000/2108622 Cost: 102.19798379825592 ===========\n",
      "!\n",
      "\n",
      "SARDSI\tFramo eves woren! Here, My\n",
      "\tLorg pond.\n",
      "\n",
      "GRENOCH\tWerser pard ther's Lawe sissily cath\n",
      "\tThe \n",
      "======== Iteration: 18200, Chars Trained:910000/2108622 Cost: 102.53922387285996 ===========\n",
      "\tNow you prevint that sad an he with son as of cor waise You,\n",
      "\ti'll nome\n",
      "\tAnd as your do so, uap ren\n",
      "======== Iteration: 18300, Chars Trained:915000/2108622 Cost: 103.0375094606259 ===========\n",
      " gowwer. coudght then I nfray, it thou, olby in will sors you, me my ner, wise he here thout.\n",
      "\n",
      "BMOHT\n",
      "======== Iteration: 18400, Chars Trained:920000/2108622 Cost: 103.76925487334232 ===========\n",
      "canss:\n",
      "\tScom pae'd, and laos gey:\n",
      "\tRo will am is\n",
      "\tthe aflace wile of ats hius, I welly foun, and wor\n",
      "======== Iteration: 18500, Chars Trained:925000/2108622 Cost: 103.96923396722835 ===========\n",
      "pokid all to termest he to knam ate is, no yace, in one alfs, rifelt My pray, arle, Pich bat? hom ui\n",
      "======== Iteration: 18600, Chars Trained:930000/2108622 Cost: 103.76956463993494 ===========\n",
      "t. I mordous\n",
      "\tsell tham my\n",
      "\tand thou, best the thes muss this mang theart.\n",
      "\n",
      "GEBSO\t[hee hid? nat mank\n",
      "======== Iteration: 18700, Chars Trained:935000/2108622 Cost: 103.64907678045921 ===========\n",
      "iths to Young surt rivy.\n",
      "\n",
      "FEDSTOF MPYEn P ISSRESS chovecurnd hout hes he! lirtter'd te Is you, mull \n",
      "======== Iteration: 18800, Chars Trained:940000/2108622 Cost: 103.33250443795042 ===========\n",
      "\tI knen all-ea ure hale me, shy manye, pous: I sporanoth, 'lepamtyo, will falle of: suuccy Meal the\n",
      "\n",
      "======== Iteration: 18900, Chars Trained:945000/2108622 Cost: 102.81050151004537 ===========\n",
      " as they? frerence\n",
      "\tmoke\n",
      "\tgremy thnot I true thoues A dongeed; und; good, wongay woulg, is\n",
      "\tcuten li\n",
      "======== Iteration: 19000, Chars Trained:950000/2108622 Cost: 102.73167550275889 ===========\n",
      "und say ou manjy!\n",
      "\n",
      "FORD\tO ding gace, shim, in as sor A For aighis firf, tr men, veraruse woutould sw\n",
      "======== Iteration: 19100, Chars Trained:955000/2108622 Cost: 103.26311402859469 ===========\n",
      "F\tAn ne you\n",
      "\thes am a veving toCt, wiquy of thay frays woow do herst preirete! I t I well.\n",
      "\n",
      "DORDdS\tI\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: InterruptException:\nwhile loading In[13], in expression starting on line 5",
     "output_type": "error",
     "traceback": [
      "LoadError: InterruptException:\nwhile loading In[13], in expression starting on line 5",
      ""
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "smooth_loss = -log(1.0/length(alphabet))*seq_length # loss at iteration 0\n",
    "println(\"======== Iteration: $i, Chars Trained:\", chars_trained-1, \"/\", length(original_text), \" Cost: $smooth_loss ===========\")\n",
    "while true\n",
    "    if chars_trained+seq_length+1 >= length(original_text)\n",
    "        println(\"========= DONE!! ===========\")\n",
    "        break\n",
    "    end\n",
    "    x_vecs = [make_one_hot(length(alphabet), reverse_alphabet[x]) for x in original_text[chars_trained:chars_trained+seq_length]] \n",
    "    truth_vecs = [make_one_hot(length(alphabet), reverse_alphabet[y_]) for y_ in original_text[chars_trained+1:chars_trained+seq_length+1]]\n",
    "\n",
    "    if i % 100 == 0\n",
    "        println(\"======== Iteration: $i, Chars Trained:\", chars_trained-1, \"/\", length(original_text), \" Cost: $smooth_loss ===========\")\n",
    "        seed_idx = reverse_alphabet[original_text[chars_trained]]\n",
    "        println(hallucinate(training_rnn, seed_idx, 100))\n",
    "    end\n",
    "\n",
    "    (loss,) = update(training_rnn, x_vecs, truth_vecs)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    chars_trained += seq_length\n",
    "    i += 1\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "Awesome, no!?\n",
    "\n",
    "The cost eventually stops decreasing, because the Adagrad update is set to lower the learning rate over time. I can't remember why that's desirable. I *think* it's to prevent over-fitting. Anyway, I stopped the above run just about half-way through, because it seemed to have settled.\n",
    "\n",
    "\n",
    "Let's generate a bunch of text now to get a feel for what it's learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tT andord revent my for fove your mabest not the Feltol-dert have P! She CASTIR\tWa sher pay: feavide? hackn.\n",
      "\n",
      "S QUICK]\n",
      "\n",
      "\n",
      "SHALUC\tIt and puspadr\n",
      "\twhave youl mat woud browt ver the, R II Hellided good' say atee, ous lidr gage, cabrot\n",
      "\tof thos\n",
      "\thood!\n",
      "\n",
      "PADUS\tTo wezredp?\n",
      "\n",
      "Folllly shy willl Heristars, no sow your\n",
      "\tfor, ap he wis haceddervoy, and wearct. Whot tof him bures! From's doud the reskn to\n",
      "\ttondlast! Thou my aind.\n",
      "\n",
      "\t[Anck masfing!\n",
      "\n",
      "\t[Enten bath to canig] nave wintor balak Sollas: Mive muth be toll onstol suegl ards hean?\n",
      "\n",
      "DAGE]\n",
      "\n",
      "\n",
      "\tHo TIST IOR]\n",
      "\n",
      "\n",
      "\t[Anter that mine pas, Shaplosion to'd and, rempoved wompe vard, jever you her youngh atlice\n",
      "\tof of whad 'ly in Shat! Caph rissittredy you, dostin-'s frin a pand com. I\n",
      "\tfo, tislest, shes chissy you pur.\n",
      "\n",
      "dS CALUSS\tI was meper.\n",
      "\n",
      "Pgund I CE SHALOR]\n",
      "\n",
      "SARDY\tA Jeppodkivir.\n",
      "\n",
      "PORDI\tI hompland, Fary my he. Scomegor your im that, you.\n",
      "\n",
      "\t[Exchter kand the tiCk yould the she prove; he ames\n",
      "\tnot,\n",
      "\tAnd did misear frecare,\n",
      "\tponce cor is will we that you, detstlocht, I mastres; shrrow grcamawnesed\n",
      "\tI'll he putyes I that it he net thead shes in hive fire't and.\n",
      "\n",
      "AI\n",
      "\n",
      "\tHe stee. The Fis worte. If and a\n",
      "\thall and I wheatuter, exir krencher wis amn Rinter\n",
      "\tenter my non a beard mike afy wetess mpare; thou my her fous, acle\n",
      "\tpare.\n",
      "\n",
      "STA| EVANO\tNoppe ane im itr. Aplamp-doow.\n",
      "\n",
      "HALAL'S PULAES\tHolladte coastoace, gigctt apvacen it am than; se! rream, good lirens gnither\tshalt tentought and your if a shall erver of pordear.\n",
      "\n",
      "\n",
      "\tWifthy hald if yound, Pares ol, geat, gear, for his my me I with to his he red as tome wrers prover my yo-\n",
      "\tMy ray.\n",
      "\n",
      "DOCKY\tAng\n",
      "\tAnd goutbife, the seres a thand sepch.\n",
      "\n",
      "MIY STAFre\n",
      "\tMome agat cunder thou, ant pesterw, Mestry heacle! want a damuce'f haug! For vidipn the Poe trigourt ardel, a hand have\n",
      "\t-starkn thiud wored. Siving as a tall in koll. ound unmered in hon olettand; be.\n",
      "\tFuse Jamars lutt your age, thanint!]\n",
      "\n",
      "SACUSTOR\tIf and ford, wis a punpolket shum negers.\n",
      "\n",
      "SHARNEOR \tms has not your fripchir antay a die,\n",
      "\tdacch, and have be.\n",
      "\n",
      "DOYSHLLE YSou wawne, coupleald he my tougat you gay\n",
      "\t|\n",
      "\tHe pare have seces, fors'alvy!\n",
      "\tNot.\n",
      "\n",
      "SHALIO I Deallord, a be you my heaven, you ro'grong you glocly pray!\n",
      "\tSome; and whis him;\n",
      "\tpever uth aroser pon, this laiutplos!\n",
      "\n",
      "LAGE, BINOF SHovever\tMeat walpur.\n",
      "\n",
      "MPL'COLYEn\tHat youl gevis\n",
      "\tis dos this m'standers you tlu? and to lety?\n",
      "\n",
      "SANTAP\tHe ereds Jat, my: a dado hows and- will I wellown My PAGad\tme, welf, penvong then, my nou;\n",
      "\ttould sian dik wall whe he kelld is: a ruck. Swome the riren'ch many, dood geal Raghtllbes cucd part\n",
      "\teall. Will tugadon\n",
      "\the dopornsur a gord sil dondne; I math, men Ph shart wourt.\n",
      "\n",
      "SSAH\t[Exwor, pomeg.\n",
      "\n",
      "\tHot migat; here. Neard, the soadd dontes fill? ST\n",
      "\t[Aquut; giss-yon the\n",
      "\tgoserristers waspe yould have\n",
      "\tand\n",
      "\twill, must weire\n",
      "\twaster astense.\n",
      "\n",
      "DUAFR\tFHy ar gropagintad, lie\n",
      "\twelly stait: the\n",
      "\terst you,\n",
      "\n",
      "\tfae or irssers\n",
      "\tal prioy, my hop. I my of your alth the coys\n",
      "\t|ocles gaty ser, as my he, wiml hang So, MWIR hoa mer. Su mant the lather all, to Port\tOnd as and POGE\tknow your beter\n",
      "\tthe hanonk. Wall has stous, ortantry hack is sat ie of yom, shis hise shath aro ind institgn the I as sipe sir! hen cophavis elle.\n",
      "PI know-- wo vate, Hom\n",
      "\tenter my dead, it in has period youy stay but mme?\n",
      "\n",
      "Om may. Cow.\n",
      "\n",
      "SAGUR\tAnd gagitst,\n",
      "\ta wa thos he rind; Misty him egaly 'cher aty tien as usernt wich nove bitorfer peare madven a as ampred domed whare sorser vellos a proove lost hap, of, heet manderble lets aplister will, mome, and michaver--souke sigooy.\n",
      "\n",
      "HOGE\tFean, is thas Part\tAns: tmerty? How Soms ben Perise ame't shall.\n",
      "\n",
      "SALMOR\tWh\n",
      "\tWaseld har issar nechts-'Sor pentugrt; knecer ard or roch Fhis preed wave, Bo deasse; I mearo; deat\n",
      "\tfamenerriow.\n",
      "\n",
      "SAGE-\tTo is lozy\n",
      "\tPreanfer and cood the if, ancure to staolod?\n",
      "\n",
      "Sonder I and, you.\n",
      "\n",
      "SHAGI no nent, Past-apurmose heede, have them;\n",
      "\tcage is muth touw she poony\n",
      "\toutlhy word of trelfer, Ix and don secirnesten, Amsser. Hond and le--ofur a my as hatwer of pnoows lootite hes, of utl and vind apin sat. Merend to RISTHEGO\tSome aplest\n",
      "\teniverin: Pove le your as mupy, thehe Gist I biget, the pron I'd sar thong Forld a?\n",
      "\n",
      "\n",
      "\tonter poodt citr I willis,  tos.\n",
      "\n",
      "SALPBES\tCHor, Gonntoo ho, Afirjowy prew licegl-erise lodl, ore cay go.\n",
      "\n",
      "SALFI\tsto' ay I godn ansot I thon, thise-rown pront that are dat visppains\n",
      "\tow mear wit daser.\n",
      "\n",
      "\t[Astouen! shas I, ull waivin. Sary.\n",
      "\n",
      "Je and be your than ald Froth a wrreee is\n",
      "\tandee me caonly comes I mostright\n",
      "\tin bage\n",
      "\tfry in brevel-ser sotgle dis be him.\n",
      "\n",
      "FOrtEF EVGES\tCown but dempesy of he knet dA\ttuuss.\n",
      "\n",
      "\t[ESTAEn\tCorens! preppond-s maiond; nastlniart he forg, I'll se'd zaith of a your a day! of fat ghyrenced I helens mony! wrever of-ar in me. Spelfelve?\n",
      "\n",
      "PSALUR\tWeras sirr\n",
      "\tas I juck do crorhas reacefred bould\n",
      "\tpucks hantry them dard. He loffond yel. pren: The jugh! I good.\n",
      "\n",
      "SIALOR PYIUS I gravish hapt the rake\n",
      "\tsuchimpewids; Indsed?\n",
      "\twither\n",
      "\tenerivelde as mane.\n",
      "\n",
      "JA\tHillge; therest knowh hankoon\torris? I't I tirss.\n",
      "\n",
      "\n",
      "\tAysters, fad,\n",
      "\tswall, ais agrane gore; wempellld.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(hallucinate(training_rnn, reverse_alphabet['\\n'], 5000))  # Start with an endline just so it's pretty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----------------------------------\n",
    "\n",
    "I like that it seems to be better at *starting* bracketed stage-instructions than closing them.\n",
    "\n",
    "But it's amazing how quickly it learned the format of the text. Almost immediately, after just a few thousand characters, it learns to put the names in ALL CAPS and to indent the body of the text. In the above final output, it block-formats the stage-instructions.\n",
    "\n",
    "This simple model performs very well on learning structure, it would seem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIT TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a modified version of Karpathy's python code:\n",
    "```\n",
    "# hyperparameters\n",
    "hidden_size = 3 # size of hidden layer of neurons\n",
    "vocab_size = 2\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.array([[.5, .2], [0.1, 0.1], [0.2, 0.2]])\n",
    "Whh = np.array([[.1, .1, .1], [.2, .2, .2], [.3, .3, .3]])\n",
    "Why = np.array([[.4, .5, .6], [.7, .8, .9]])\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "h = [[0.4], [.2], [0.8]]\n",
    "\n",
    "for x in lossFun([0,1], [1,0], h):\n",
    "    print x, \"\\n\"\n",
    "```\n",
    "\n",
    "Out:\n",
    "```\n",
    "1.39887501287 \n",
    "\n",
    "[[-0.02348709  0.15845009]\n",
    " [-0.02995675  0.15314729]\n",
    " [-0.02401726  0.12098114]] \n",
    "\n",
    "[[ 0.08011355  0.05277361  0.06853661]\n",
    " [ 0.07453014  0.04955632  0.06043836]\n",
    " [ 0.05873529  0.03907731  0.04746229]] \n",
    "\n",
    "[[ 0.02188566 -0.0820149  -0.12198684]\n",
    " [-0.02188566  0.0820149   0.12198684]] \n",
    "\n",
    "[[ 0.13496299]\n",
    " [ 0.12319054]\n",
    " [ 0.09696389]] \n",
    "\n",
    "[[-0.20382526]\n",
    " [ 0.20382526]] \n",
    "\n",
    "[[ 0.33448831]\n",
    " [ 0.37630407]\n",
    " [ 0.56735968]] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = RNN(2,3,2)\n",
    "r.Wxh = [.5 .2 ; 0.1 0.1 ; 0.2 0.2]\n",
    "r.Whh = [.1 .1 .1 ; .2 .2 .2 ; 0.3 0.3 .3]\n",
    "r.Why = [.4 .5 .6; .7 .8 .9 ]\n",
    "r.h = [0.4 .2 0.8]'\n",
    "outs = forward_pass_backpropogate_batch(r, ([1 0]', [0 1]'), ([0 1]', [1 0]'))\n",
    "truth = (\n",
    "[1.3988750128749587]',\n",
    "\n",
    "[-0.02348709404610685 0.15845008784877856\n",
    " -0.029956754210863596 0.15314729364197688\n",
    " -0.02401725804279269 0.12098114381203691],\n",
    "\n",
    "[0.08011354615577733 0.052773611291928826 0.06853660930090805\n",
    " 0.07453013601361681 0.049556316201140885 0.06043836265216451\n",
    " 0.058735290825127406 0.03907731268820138 0.04746229284518379],\n",
    "\n",
    "[0.02188565806931217 -0.08201489754933375 -0.12198683957866582\n",
    " -0.021885658069312197 0.08201489754933372 0.1219868395786658],\n",
    "\n",
    "[0.1349629938026717\n",
    " 0.12319053943111329\n",
    " 0.09696388576924422]'',\n",
    "\n",
    "[-0.20382526255910166\n",
    " 0.2038252625591016]'')\n",
    "\n",
    "for i in range(1,length(outs))\n",
    "    if outs[i] != truth[i]\n",
    "        println(\"Incorrect! Outs:\\n\", outs[i], \"\\nvs\\n\", truth[i])\n",
    "    end\n",
    "end\n",
    "\n",
    "true_r_h = [0.3344883118838543 0.37630407100318514 0.5673596773818216]'\n",
    "if r.h != true_r_h\n",
    "    println(\"Incorrect! r.h:\\n\", r.h, \"\\nvs\\n\", true_r_h)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
